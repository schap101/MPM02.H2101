---
title: "MPM.02 Groupwork"
output: html_notebook
author: "Felix, Regis, Sofie, Philipp"
date: 06/01/2022
output:
  html_document:
    toc: yes
    toc_float: no
    toc_depth: 4
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

Sofie used {r setup, include=FALSE}
````{r, echo= FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load library
library(knitr)    # For knitting document and include_graphics function
library(png)
library(rstudioapi)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(rstudioapi)
library(caret)
library(GGally)
library(magrittr)
library(mosaicCore)
library(readr)
library(Metrics)
library(data.table)
library(e1071)
library(kernlab)
library(cowplot)
library(tidyverse)
library(neuralnet)
library(rstudioapi)
library(rmarkdown)
```

````{r, echo=FALSE}
# set directory read data
setwd(dirname(getActiveDocumentContext()$path))
df <- read.csv("../Data/hour.csv")
`````


# Introduction

In this project work for machine learning 1, we would like to examine a dataset of bicycle rental numbers and build different models to predict the number of rentals at a given time.

Obviously, the number of rentals depends on the weather as well as on temporal or seasonal factors. These factors will be investigated.

The historical data is from the Capital BikeShare system for Washington D.C. USA.The data is available for the year 2011 to 2012.The dataset also contains weather data at the given time points.

# Graphical Data Exploration

## Influnece of Weather upon Bike Rentals

In this chapter we would like to explore how different weather conditions influence the bike rental numbers.

### Exploration of Continuous Variables

We want to explore how different weather situation influences the number of bike rentals. For that we have following 4 continuous weather variables at hand which we are going to examine:

- temp:       Normalized temperature in Celsius. t_min [-8], t_max [+39]
- atemp:      Normalized feels like temperature in Celsius.  t_min [-16], t_max [+50]
- hum:        Normalized humidity. The values are divided with max. of 100 
- windspeed:  Normalized wind speed. The values are divided with min. of 67 

The continuous weather variables are normalized as follows in the dataset:

````{r,echo=FALSE}
str(df)
summary(df)
```

$$
x_{normalized} = \frac{x-x_{min}}{x_{max}-x_{min}}
$$
For some models it may be useful to use the normalized variables and for other we may re-transform the variables. If so, we will state that in the coressponding chapter.

```{r,echo=FALSE}
p1 <-  ggplot(data = df,
         mapping = aes(y = log(cnt),
                     x = temp)) + xlab("Normalized Temperature") + ylab("Num. of Bike Rentals") + 
     geom_point(alpha = 0.1,shape = 1) +
     geom_smooth(aes(colour = "gam")) +
     geom_smooth(aes(colour = "lm"), method = "lm") +
     labs(colour = "Method")

p2 <- ggplot(data = df,
         mapping = aes(y = log(cnt),
                     x = atemp)) +  xlab("Normalized Feels Like Temperature") + ylab("") +
     geom_point(alpha = 0.1,shape = 1) +
     geom_smooth(aes(colour = "gam")) +
     geom_smooth(aes(colour = "lm"), method = "lm") +
     labs(colour = "Method")

p3 <- ggplot(data = df,
         mapping = aes(y = log(cnt),
                     x = hum)) + xlab("Normalized Humidity") + ylab("Num. of Bike Rentals") +
     geom_point(alpha = 0.1,shape = 1) +
     geom_smooth(aes(colour = "gam")) +
     geom_smooth(aes(colour = "lm"), method = "lm") +
     labs(colour = "Method")

p4 <- ggplot(data = df,
         mapping = aes(y = log(cnt),
                     x = windspeed)) + xlab("Normalized Windspeed") + ylab("") +
     geom_point(alpha = 0.1,shape = 1) +
     geom_smooth(aes(colour = "gam")) +
     geom_smooth(aes(colour = "lm"), method = "lm") +
     labs(colour = "Method")

grid.arrange(p1,p2,p3,p4, nrow=2,top = "Continuous Weather Variables vs. Number of Bike Rentals")

fig.align = 'center'

```


In all 4 scatter plots the linear model lines (green) as well as the gam model lines (red) are plotted. For both temperature variables the rentals seems to increase until the temperature reaches a certain value.At the beginning of the observations (low temp.) ans well as the end (high temp.) the linear model seems to overestimate the number of rentals.
For the normalized humidity there is a decrease in rentals with increasing humidity. The is not the case between 0 and 25 % humidity where the linear model overestimates the number of rentals. We assume that in very cold and very hot weather the humidity is rather low and so the number of bike rentals because of the influence of temperature.
In conclusion of the above exploration, it can be stated that the behavior cannot be completely represented with a linear model. There are nonlinear aspects which should be considered in the prediction models.
In the next chapter we would like to examine the categorical variables which are available in the dataset.

### Exploration of Categorical Variables

The data set contains 4 different weather categories which are described as follows:

- Category 1:      Clear, Few clouds, Partly cloudy, Partly cloudy
- Category 2:      Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- Category 3:        Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 
- Category 4:  Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 

Since we are considering the response variables count data we log transform the variable with the natural logarithm in all of the following plots.
````{r,echo=FALSE}

ggplot(data = df, mapping = aes(y = log(cnt),
                     x = factor(weathersit),fill= weathersit)) + ylab("Num. of Bike Rentals") + xlab("Weather Conditions") +  labs(fill="Weather\nConditions") +
geom_boxplot() 


```
The boxplot above indicates, that in situation 1 and 2 the most bikes are rented whereas there is seems to be a drop visible  for condition 3 and the drop is even more significant for situation 4 where severe weather conditions are present.This rental behavior is as we have expected since few people ride bicycles in bad weather situations.
In the next chapter we are inspecting the seasonal and timely influence upon the rental situation.

## Influnece of Timely and Seasonal aspects upon Bike Rentals

### Exploration of Categorical Variables

#### Seasonal Influence 
In order to investigate the seasonal influences, the monthly (1 to 12) and seasonal data are included in the dataset as shown below::

- Category 1:      Spring
- Category 2:      Summer
- Category 3:      Fall
- Category 4:      Winter

````{r,echo=FALSE}

p1 <- ggplot(data = df,  mapping = aes(y = log(cnt),
                     x = factor(season),fill= season)) + ylab("Num. of Bike Rentals") + xlab("Season") +  labs(fill="Season") +
geom_boxplot() 


p2 <- ggplot(data = df,
       mapping = aes(y = log(cnt),
                     x = factor(mnth),fill= mnth)) + ylab("Num. of Bike Rentals") + xlab("Month") +  labs(fill="Monthn") +
geom_boxplot()
grid.arrange(p1,p2, nrow=2,top = "Seasonal Aspects vs. Number of Bike Rentals")

fig.align = 'center'
```

The two boxplot above indicate, that the mos bike rentals take place in summer and fall which we have expected.

#### Timely Influence 

In order to examine the temporal influences we have the time specification in hours (1 to 24), the weekdays (0 = Monday to 6 = Sunday), working day (1 = yes, 0 = no) as well as vacation day (1 = yes , 0 = no).


````{r,echo=FALSE}

p1 <- ggplot(data = df,
       mapping = aes(y = log(cnt),
                     x = factor(hr),fill= hr)) + ylab("Num. of Bike Rentals") + xlab("Hour") +  labs(fill="Hour") +
geom_boxplot() 

p2 <- ggplot(data = df,
       mapping = aes(y = log(cnt),
                     x = factor(weekday),fill= weekday)) + ylab("Num. of Bike Rentals") + xlab("Weekday") +  labs(fill="Day") +
geom_boxplot()

p3 <- ggplot(data = df,
       mapping = aes(y = log(cnt),
                     x = factor(workingday),fill= workingday)) + ylab("Num. of Bike Rentals") + xlab("Workingday") +  labs(fill="Day") +
geom_boxplot() 

p4 <- ggplot(data = df,
       mapping = aes(y = log(cnt),
                     x = factor(holiday),fill= holiday)) + ylab("Num. of Bike Rentals") + xlab("Holidays") +  labs(fill="Day") +
geom_boxplot() 

grid.arrange(p1,p2,p3,p4, nrow=2,top = "Timely Aspects vs. Number of Bike Rentals")


```

The above plots indicate, that the hour in which the bike is rented has the most influence of total rentals which seems obvious. On the other hand, whether there is a working day or not, seems not to have a significant influence.

In the next chapter we try to find a linear model which suits best the dateset.
# Linear Regression Model
## Data Preperation for the Linear Model
### Set Categorical Variables as Factors
```{r}
df$season <- factor(df$season, levels = c("1", "2", "3", "4"), ordered = FALSE)
df$yr <- as.factor(df$yr)
df$mnth <- as.factor(df$mnth)
df$hr <- as.factor(df$hr)
df$holiday <- as.factor(df$holiday)
df$weekday <- as.factor(df$weekday)
df$workingday <- as.factor(df$workingday)
df$weathersit <- as.factor(df$weathersit)
```
### Re-Transform the Continious Weather Variables

As described in the data exploration chapter, the continuous weather variables have been normalized in the existing dataset. In order to make interpretation of the coefficients more straight forward in the linear model the variables will be re-transformed to the actual values as follows. For more advanced models the normalized variables may be the better choice to fit a model.
```{r}
df$temperature <- (df$temp * (39 + 8)) - 8
df$atemperature <- (df$atemp * (39 + 8)) - 8
df$humidity <- (df$hum * 100)
df$wind <- (df$windspeed * 67)
```
### Split data into train and test dataset

In the following the dataset will be split into a train and a test dataset. The partition will be used to evaluate the accuracy of the model when used on the test data.
````{r}
set.seed(123)
indices <- createDataPartition(df$cnt, p=.8, list = F)
train <- df %>% slice(indices)
test <- df %>% slice(-indices)
```

## Multiple Linear Regression Model with Continious Weather Predictors
First of all we want to include only the continuous variables in the model.
```{r}
lm.fit.0.train <- lm(log(cnt) ~ temperature + atemperature + humidity + wind,data=train)
summary(lm.fit.0.train)
```

### Simplification of Model lm.fit.0.train

There is evidence that the predictor temperature has no significant influence since the p-value is large with 0.7. However the feels like temperature (atemperature) is a result of the combination of the temperature, the humidity and the wind at a given time. Moreover, the prediction of future bike rental will depend from the weather forecast. In the forecast the feels like temperature is not always available. Therefore we try to fit the model without atemperature to simplify it and make it more practical for future usage.

```{r}
lm.fit.1.train <- lm(log(cnt) ~ temperature + humidity + wind,data=train)
summary(lm.fit.1.train)
```
Interpretation of Regression Coefficients:

- As we can see from the output above the predictor temperature and wind have both a positive influence upon the number of bike rentals. A increase of one unit (temperature) leads to a increase of 5.9 % rentals and 0.6 % when the wind is increased by one unit. The humidity on the other hand has a negative influence. If the humidity is increased by one unit the number of rentals decreases by 2.3 %.


### Accessing the model accuracy
We evaluate the accuracy of both models by testing them with the test data partition. The R-squared will serve as an indication of the accuracy as well as the RSME values.

#### Compare R-squared Values
```{r}
# Fit a linear model without the predictor atempereature and accessing both prediction performances with the test data and comparison of the R-squared value.

lm.fit.0.test = lm(formula(lm.fit.0.train),data=test)
lm.fit.1.test = lm(formula(lm.fit.1.train),data=test)

R2_0 <- summary(lm.fit.0.test)$r.squared
R2_1 <- summary(lm.fit.1.test)$r.squared

print(paste("R-squared of lm.fit.0:",round(R2_0,3)))
print(paste("R-squared of lm.fit.1:",round(R2_1,3)))
```
- Both R-squared values are almost identical when applying the model on the test data. Therefore, the predictor atemperature shall be neglected for the linear model in the further model building.

#### Compare RMSE Values
```{r}
control <- test$cnt
control_log <- log(control)
predict_0 <- predict(lm.fit.0.test)
predict_1 <- predict(lm.fit.1.test)

# take inverse of natural log to obtain RMSE with actual unit (number of bike rentals)
RMSE_0 <- sqrt(mean((exp(control_log) - exp(predict_0))^2))
RMSE_1 <- sqrt(mean((exp(control_log) - exp(predict_1))^2))

# calculate percentage error
percent_error_0 <- (RMSE_0/mean(control))*100
percent_error_1 <- (RMSE_1/mean(control))*100

print(paste("RMSE of lm.fit.0:",round(RMSE_0,1)))
print(paste("Percentage error of lm.fit.1:",round(percent_error_0,1),"%"))
print(paste("RMSE of lm.fit.0:",round(RMSE_1,1)))
print(paste("Percentage error of lm.fit.1:",round(percent_error_1,1),"%"))
```
- The RMSE values for both models are roughly 170 units as we can see from the output above. This corresponds to a percentage error of around 90 % of the predicted values if it is compared with the mean of all bike rentals.

## Multiple Linear Regression Model with Continious and Categorical Variables

```{r}
# Update the previous model lm.fit.1.train, including all the categorical variables.
lm.fit.2.train <- update(lm.fit.1.train,. ~ . + weathersit + hr + season + mnth + workingday + weekday + holiday + yr ,data=train)
formula(lm.fit.2.train)
```

### Simplification of Model lm.fit.2.train

In the next step we try fo find again a more simple and more practical model without reducing the predicting accuracy of the model significantly. With the drop1 function we test influence of the categorical variables with a F test as follows.

```{r}
drop1(lm.fit.2.train, test = "F")
```
There is evidence that the variables workingday an holiday have no significant influence according the results of the F-test. We therefore neglect this variables in the model lm.fit.3.train. The graphical analysis in the last chapter supports this step, showing that there is no influence of these variables.
Moreover the dataset contains of the variable year (yr). This variables indicates whether we are in the year 2011 or in 2012. This variable however is useless in terms of a predicting model. Hence we will neglect it in the model lm.fit.4.train.
```{r}
lm.fit.3.train <- update(lm.fit.1.train,. ~ . + weathersit + hr + season + mnth + weekday + yr ,data=train)
formula(lm.fit.3.train)
lm.fit.4.train <- update(lm.fit.1.train,. ~ . + weathersit + hr + season + mnth + weekday ,data=train)
formula(lm.fit.4.train)
```
### Accessing the model accuracy

#### Compare R-Squared Values

Again we compare the R-squared values of the 3 models.
```{r}
lm.fit.2.test = lm(formula(lm.fit.2.train),data=test)
lm.fit.3.test = lm(formula(lm.fit.3.train),data=test)
lm.fit.4.test = lm(formula(lm.fit.4.train),data=test)

R2_2 <- summary(lm.fit.2.test)$r.squared
R2_3 <- summary(lm.fit.3.test)$r.squared
R2_4 <-summary(lm.fit.4.test)$r.squared

print(paste("R-squared of lm.fit.2:",round(R2_2,3)))
print(paste("R-squared of lm.fit.3:",round(R2_3,3)))
print(paste("R-squared of lm.fit.4::",round(R2_4,3)))
```
- Form the previous model the the most simplified we can see only a marginal drop of the R-squared values. Therefore, it can be stated that the simplified model can be used without losing considerable information. 

#### Compare RMSE Values
```{r}
predict_2 <- predict(lm.fit.2.test)
predict_3 <- predict(lm.fit.3.test)
predict_4 <- predict(lm.fit.4.test)

# take inverse of natural log to calculate RMSE with actual unit (number of bike rentals)
RMSE_2 <- sqrt(mean((exp(control_log) - exp(predict_2))^2))
RMSE_3 <- sqrt(mean((exp(control_log) - exp(predict_3))^2))
RMSE_4 <- sqrt(mean((exp(control_log) - exp(predict_4))^2))

# Calculate percentage error (deviation to the mean of number of bike rentals)
percent_error_2 <- (RMSE_2/mean(control))*100
percent_error_3 <-(RMSE_3/mean(control))*100
percent_error_4 <-(RMSE_4/mean(control))*100

print(paste("RMSE of lm.fit.2:",round(RMSE_2,1)))
print(paste("Percentage error of lm.fit.2:",round(percent_error_2,1),"%"))
print(paste("RMSE of lm.fit.3:",round(RMSE_3,1)))
print(paste("Percentage error of lm.fit.3:",round(percent_error_3,1),"%"))
print(paste("RMSE of lm.fit.4:",round(RMSE_4,1)))
print(paste("Percentage error of lm.fit.4::",round(percent_error_4,1),"%"))
```
- The RMSE values for both 3 models are between 97 t0 110 units as we can see from the output above. This corresponds to a percentage error of around 51 to 59 % of the predicted values if it is compared with the mean of all bike rentals.

```{r}
summary(lm.fit.4.test)
```
- Note from the output above, that the p-values for some month are not significant. More on that on in the next chapter.

## Summary
As summary we want to answer following 4 questions regarding the linear regression model and its interpretation.

### Is there a relationship betweeen the independent variables and the response variable?

As stated above there is evidence, that the weather as well as the timely and seasonal variables has influence of the number of bike rentals according to the P-values for the continuous variables as well as the outcome of the F-test for the categorical variables.

### How strong is the relationship and how accurate is the model?

The predictors used in the model lm.fit.4 explain about 80 % of the variance in cnt (number of bike rentals) according to the R-squared values. If we look at the RMSE value of the lm.fit.4 we achieve a value of 110.5 which corresponds to a percentage error of around 60 %.

### How large is the effect of each predctor on cnt?

To answer this question we will plot the confidence interval of all the predictors.

```{r}
ggcoef(lm.fit.4.train,vline_color = "red", vline_linetype =  "solid", errorbar_color = "blue", errorbar_height = .25)
```
We can see that for the predictor month all dummy variables are crossing the zero line, indicating that this variable is not statistically significant. Therefore we could have dropped this predictor as well even though the p-values where low in the model examination. The evaluate this assumption we update the model and compare the two R-squared values.

```{r}
lm.fit.5.train <- update(lm.fit.1.train,. ~ . + weathersit + hr+ season + weekday ,data=df)
lm.fit.5.test = lm(formula(lm.fit.5.train),data=test)

summary(lm.fit.4.test)$r.squared
summary(lm.fit.5.test)$r.squared
```
In fact, the R-squared value decreases only marginally when the predictor month is not taken into account, even though the F-statistic has classified the predictor as significant.
Finally, it can be concluded that the temperature as well as the current time have the greatest effect upon the number of bike rentals, since the confidence intervals are the furthest away from zero.

### Are there Potential Problems of Fitting this data with a Linear Regression Model 

With the plot function, we try to examine our model in more detail by means of residual analysis.To to that we build a model with the entire dataset.
````{r}
lm.fit.0.full = lm(formula(lm.fit.4.train),data=df)
par(mfrow=c(2,2))
plot(lm.fit.0.full)
`````

- The residual vs fitted plot shows no clear pattern indicating that that relationship is linear. Also the smoother stays more or less on zero. In a clear non-linear situation this would have been the case.

- The QQ-plot shows that the residuals seem not to follow a normal distribution. This circumstance could be a sign that the linear model is not quite suitable to form a predictive model. We assume that this is due to the distribution of bike rentals. These are not normally distributed and could be considered as amount data.

- Checking the scale-location plot, it seems that the residuals are spread fairly randomly along the horizontal line. Altough the variance seems to decrease on the right hand side of the plot we consider this as "normal".

- In the residual vs leverage, one observation seems to lie outside of the cooks distance. It is the observation 5637 in the dataset. A closer look reveals an error in the dataset. On 27.08.2011 and 28.08.2011 the time seems to be incorrect and therefore the observation 5637 can be considered as an outlier. It would probably make sense to delete these two days from the dataset. We do this in the following step and form a new model.

````{r}
df2 <- df[!(df$dteday=="2011-08-27" | df$dteday=="2011-08-28"),]
lm.fit.1.full = lm(formula(lm.fit.4.train),data=df2)
plot(lm.fit.1.full, which = 5)
summary(lm.fit.0.full)$r.squared
summary(lm.fit.1.full)$r.squared
`````
- As we can see, all observations are lying now within the cook distance and the R-value also increases marginally when taking out the faulty date entries in a updated dataset.

In the next chapter we will use more advanced linear models to find a better fit for the data.

#GLM Model
## Reading in the daily data as "day" and the hourly data as "hour" and having a look at it
```{r}
setwd(dirname(getActiveDocumentContext()$path))
day_0 <- read_csv("../Data/day.csv")
head(day_0)
```


````{r}
hour_0 <- read_csv("../Data/hour.csv")
head(hour_0)
```

For this model we are also going to drop the data from the 27th and 28th of August 2011, for the reasons shown in the last chapter.

```{r}
day <- day_0[!(day_0$dteday=="2011-08-27" | day_0$dteday=="2011-08-28"),]
hour <- hour_0[!(hour_0$dteday=="2011-08-27" | hour_0$dteday=="2011-08-28"),]
```


````{r}
str(hour)
````

````{r}
str(day)
````

## Plotting and inspecting the data
First we have a look at how the different variables are corresponding with number of bicycles, that are being used 
```{r , echo=True}
pairs.daily <- pairs(cnt ~ dteday + season + yr + mnth + holiday + weekday + workingday + weathersit + temp + atemp + hum + windspeed + casual + registered, data = day, upper.panel = panel.smooth)


```

```{r, echo=True}
pairs.hour <- pairs(cnt ~ dteday + season + yr + mnth + hr+ holiday + weekday + workingday + weathersit + temp + atemp + hum + windspeed + casual + registered, data = hour, upper.panel = panel.smooth)

```


What we can see here, is that it looks like there is an uptake in bicycle usage when
the temperature is rising (up to a certain point). When it gets too hot, the demand seems to lower again. This also corresponds in the usage during the different seasons, with the least demand in winter.

The variable for casual or registered users is less of importance here, as it does not necessarily help with the prediction of the bike demand. Especially as we don't see drastically different behaviour regarding usage. Just from the graph above one could assume, that it is slightly more likely that registered users use the bikes also in more extreme weather situations (hot, cold, windy, etc.). Furthermore, we will not use the type of users (registered or casual) as predictors, as they make up our main response variable (count) and therefore would lead to highly fitting models but with no value for prediction.

So let's have another look at the following variables:
- season
- mnth
- hour
- weathersit
- temp
- atemp
- hum
- windspeed

```{r, echo=True}
pairs.hour.new <- pairs(cnt ~ season + mnth + hr+ weathersit + temp + atemp + hum + windspeed, data = hour, upper.panel = panel.smooth)
```




```{r, echo=True}

ggplot(day,aes(x=(season= as.character(season)), y=cnt, fill=season)) + 
  geom_boxplot()+labs(x="Seasons", y="Number of Bikes used")

```
```{r, echo=True}
ggplot(day,aes(x=(mnth= month.abb[mnth]), y=cnt, fill=season)) + 
  geom_boxplot()+labs(x="Months", y="Number of Bikes used") + scale_x_discrete(limits = month.abb)

```

It looks like late winter/early spring is the least popular time for bike rentals. Let's check, if it might have something to do with the weather.

```{r, echo=True}
# First we are having a look at the temperature per month to get a better picture of the situation
ggplot(day,aes(x=(mnth= month.abb[mnth]), y=temp, fill=season)) + 
  geom_boxplot()+labs(x="Months", y="Temperature") + scale_x_discrete(limits = month.abb)

```

The temperature shows a similar, although not identical, pattern as the bike usage, with the lowest temperatures being in late winter/early spring.

```{r pressure, echo=True}
# Having a look at the wind speed per month
ggplot(day,aes(x=(mnth= month.abb[mnth]), y=windspeed, fill=season)) + 
  geom_boxplot()+labs(x="Months", y="Wind Speed") + scale_x_discrete(limits = month.abb)
```
For the wind speed we cannot say the same. 

```{r, echo=True}
# Having a look at the temperature per month
ggplot(day,aes(x=(mnth= month.abb[mnth]), y=hum, fill=season)) + 
  geom_boxplot()+labs(x="Months", y="Humidity") + scale_x_discrete(limits = month.abb)
```

Neither for the humidity. The pattern looks completely different.

According to those visualizations one would assume, that the temperature is the main predicting factor of those three.


## Creating training and testing sets

First we divide the data into a training and a testing set. We use 80% of the data to train the model and 20% to test it. Furthermore we divide the testing set into one we use for the prediction and one to check those predictions.

```{r}

set.seed(123)
indices.day <- createDataPartition(day$cnt, p = .8, list = F) 
 
day.train <- day %>% slice(indices.day) 
day.test_in <- day %>% slice(-indices.day) %>% select(-cnt) # contains everything except the count values
day.test_truth <- day %>% slice(-indices.day) %>% pull(cnt)  # contains the true count values of the testing set


set.seed(123)
indices.hour <- createDataPartition(hour$cnt, p = .8, list = F) 
 
hour.train <- hour %>% slice(indices.hour) 
hour.test_in <- hour %>% slice(-indices.hour) %>% select(-cnt) 
hour.test_truth <- hour %>% slice(-indices.hour) %>% pull(cnt)



```



## Creating a binomial model

First we have a look at a simple quasibinomial model, with the assumption that there are no interactions and all effects are linear. As we only have two binary variables, we are only going to check those two here. We use ilogit() as we need the count numbers as a value between 0 and 1.


```{r}
glm.binomial.hour.1 <- glm(ilogit(cnt) ~ dteday + season + yr + mnth + hr+ holiday + weekday + workingday + weathersit + temp + atemp + hum + windspeed, data = hour, family = "quasibinomial")

summary(glm.binomial.hour.1)
```


We decided to take out the holidays, as there are only very few anyways, the assumed temperature (atemp) as it is very close to the temperature (temp) and also dependent on the temperature, humidity and wind speed. We also took out the year.

```{r}
glm.binomial.hour.2 <- glm(ilogit(cnt) ~ dteday + season + mnth + hr + weekday + workingday + weathersit + temp + hum + windspeed, data = hour, family = "quasibinomial")

summary(glm.binomial.hour.2)
```
Interestingly the month does not seem to have a significant impact. All the other predictors show a high significance.


### Fitting the binomial model with "train" data 

For the binomial model we use the quasibinomial family, as the data is overdispersed and we do not use a binary, but a binomial response variable.
```{r}
glm.binomial.1 <- glm(ilogit(cnt) ~ dteday + season + yr + mnth + hr + holiday + weekday + workingday + weathersit + temp + hum + windspeed, data=hour, family="quasibinomial")

glm.binomial.train.1 <- glm(formula = formula(glm.binomial.1), data = hour.train)
```

Making a prediction based on the test data: 
```{r}
predicted.binomial.test.1 <- predict(glm.binomial.train.1, newdata = hour.test_in)
```

```{r}
summary(predicted.binomial.test.1)
```


Checking root mean square error (RMSE)

```{r}
# As we used ilogit() for the count data of the model, we need to apply it to the control data as well
rmse(ilogit(hour.test_truth), predicted.binomial.test.1)
```
It looks like there are missing (probably through applying the ilogit() function) and therefore the RMSE could not be calculated.

So let us just have look at a visualisation of our predicted values.

```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(hour.test_in$dteday,predicted.binomial.test.1,type="l",col="firebrick3",  xlab = " ", ylab ="ilogit of Number of Bikes used")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction"),col=c("firebrick3"), pch=c(1, 2), lty=c(1,2))
```

The plot of the model prediction looks like it could work, but checking it with the true data is proving difficult.
So we try a new approach.

To fit a binomial model we will try to predict the proportion of registered users versus casual users.
For this we use the percentage of registered users out of the total number of useres as the response variable.
```{r}
glm.binomial.2 <- glm((registered/cnt) ~ dteday + season + yr + mnth + hr + holiday + weekday + workingday + weathersit + temp + hum + windspeed, data=hour, family="quasibinomial")

glm.binomial.train.2 <- glm(formula = formula(glm.binomial.2), data = hour.train)
```


```{r}
summary(glm.binomial.2)
```
The following predictors seems to show significance:
- Season
- Hour
- Weekday
- Workingday
- Temperature
- Humidity

Making a prediction based on the test data:
```{r}
predicted.binomial.test.2 <- predict(glm.binomial.train.2, newdata = hour.test_in)
```

```{r}
summary(predicted.binomial.test.2)
```


```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(hour.test_in$dteday,predicted.binomial.test.2,type="l",col="firebrick3",  xlab = " ", ylab ="Percentage of Registered Users")
lines(hour.test_in$dteday,(hour.test_in$registered/hour.test_truth),type="p",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
As the hourly data varies a lot, it is difficult to see if the model matches or not.

Checking root mean square error (RMSE)
```{r}
# It is important to calculate the percentage for the true numbers as well
rmse((hour.test_in$registered/hour.test_truth), predicted.binomial.test.2)
```
With a FMSE of about 11 percentage points, the model does not look too bad.

But let's try to fit the model with daily data to bring down the variance.

```{r}
glm.binomial.3 <- glm((registered/cnt) ~ dteday + season + yr + mnth + holiday + weekday + workingday + weathersit + temp + hum + windspeed, data=day, family="quasibinomial")

glm.binomial.train.3 <- glm(formula = formula(glm.binomial.3), data = day.train)
```

```{r}
summary(glm.binomial.3)
```

Making a prediction based on the test data:
```{r}
predicted.binomial.test.3 <- predict(glm.binomial.train.3, newdata = day.test_in)
```

```{r}
summary(predicted.binomial.test.3)
```


```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(day.test_in$dteday,predicted.binomial.test.3,type="l",col="firebrick3",  xlab = " ", ylab ="Percentage of Registered Users")
lines(day.test_in$dteday,(day.test_in$registered/day.test_truth),type="l",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
Here we see our prediction in red and the true values in blue.

Checking root mean square error (RMSE)
```{r}
rmse((day.test_in$registered/day.test_truth), predicted.binomial.test.3)
```
We see that our model fitted on daily data became more accurate. With a RMSE of roughly 6 percentage points this seems like a pretty good fit, regarding the ratio depends on human decision (being registered or casual and using a bike or not).

Now we try the model with only the significant predictors from above (summary(glm.binomial.3))
```{r}
glm.binomial.4 <- glm((registered/cnt) ~season + workingday + weathersit + temp + windspeed, data=day, family="quasibinomial")

glm.binomial.train.4 <- glm(formula = formula(glm.binomial.4), data = day.train)
```

```{r}
summary(glm.binomial.4)
```


Making a prediction based on the test data:
```{r}
predicted.binomial.test.4 <- predict(glm.binomial.train.4, newdata = day.test_in)
```

```{r}
summary(predicted.binomial.test.4)
```


```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(day.test_in$dteday,predicted.binomial.test.4,type="l",col="firebrick3",  xlab = " ", ylab ="Percentage of Registered Users")
lines(day.test_in$dteday,(day.test_in$registered/day.test_truth),type="l",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
Here we see our predictions in red and the true data in blue.

Checking root mean square error (RMSE)
```{r}
rmse((day.test_in$registered/day.test_truth), predicted.binomial.test.4)
```
This seems to have made the model slightly less accurate (RMSE of 0.06142542 compared to 0.0612635 from before). But the difference seems to be minimal. Together with the result from the summary above (summary(glm.binomial.4)) the hypothesis could be made, that registered users make up a bigger part of the total count of users, when the temperature is colder and on working days compared to warmer temperatures and non-working days. Or that casual users, in comparison to registered users, predominantly use the bike service on warmer days and during their spare time.

Let's have a look at the temperature and workingday variable, which we assume has shown the greatest significance (see summary(glm.binomial.4))
```{r}
exp(coef(glm.binomial.4)["temp"])
```
A change in one temperature unit (as it is normalised in this data frame) leads to a change of roughly 16.5%.

```{r}
exp(coef(glm.binomial.4)["workingday"])
```

On working days we see the percentage of users being registered 3.08 times higher then on non-working days.


To make sure we are not missing anything, we will also fit a model to the date variables to compare. We still will keep in the working day variable, as this is not a weather variable but still was included in the former model.

```{r}
glm.binomial.5 <- glm((registered/cnt) ~ dteday + yr + mnth + holiday + weekday + workingday, data=day, family="quasibinomial")

glm.binomial.train.5 <- glm(formula = formula(glm.binomial.5), data = day.train)
```

```{r}
summary(glm.binomial.5)
```
Here we already see, that the working day is assumed as the only significant predictor in this model.

Making a prediction based on the test data:
```{r}
predicted.binomial.test.5 <- predict(glm.binomial.train.5, newdata = day.test_in)
```

```{r}
summary(predicted.binomial.test.5)
```

```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(day.test_in$dteday,predicted.binomial.test.5,type="l",col="firebrick3",  xlab = " ", ylab ="Percentage of Registered Users")
lines(day.test_in$dteday,(day.test_in$registered/day.test_truth),type="l",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
In this visualisation we can already see that the the model has a worse fit.


Checking root mean square error (RMSE)
```{r}
rmse((day.test_in$registered/day.test_truth), predicted.binomial.test.5)
```
Surprisingly, the RMSE is not much higher than from our predicted.binomial.test.4.
Nevertheless, we would clearly prefer the third or fourth model here.


## Creating a Poisson Model

We are using the family quasipoisson because of the overdispersion.

```{r}
glm.pois.hour <- glm(cnt ~ dteday + season + yr + mnth + hr+ holiday + weekday + workingday + weathersit + temp + atemp + hum + windspeed, data = hour,
family = "quasipoisson")

summary(glm.pois.hour)
```

Here it looks like quite some factors seem to be significant for the number of bikes used.
With the temperature and the assumed temperature being very similar, we drop the assumed temperature from the model, with the measured temperature being more tangible. 


```{r}
glm.pois2.hour <- glm(cnt ~ dteday + season + yr + mnth + hr+ holiday + weekday + workingday + weathersit + temp + hum + windspeed, data = hour, family = "quasipoisson")

summary(glm.pois2.hour)

```
Now let's look at it one more time, without the year (as we only have data from two years) and without the holidays (as there are only very few of them). 

```{r}
glm.pois3.hour <- glm(cnt ~ dteday + season + mnth + hr + weekday + workingday + weathersit + temp + hum + windspeed, data = hour.train, family = "quasipoisson")

summary(glm.pois3.hour)

```

### Fitting the poisson model with "train" data 
```{r}
glm.poisson.1 <- glm(cnt ~ dteday + season + yr + mnth + hr + holiday + weekday + workingday + weathersit + temp + hum + windspeed, data=hour, family=quasipoisson)

glm.poisson.train.1 <- glm(formula = formula(glm.poisson.1), data = hour.train)
```

Making a prediction based on the test data:
```{r}
predicted.poisson.test.1 <- predict(glm.poisson.train.1, newdata = hour.test_in)
```

```{r}
summary(predicted.poisson.test.1)
```
Note: Although we see a minimum of -130.4 here, in reality the number cannot go beneath 0.

Checking the root mean square error
```{r}
rmse(hour.test_truth, predicted.poisson.test.1)
```
Having a root mean square error (RMSE) of 139 when we have numbers of about 0 to about 500 does definitely not sound very good. But we have to keep in mind, that it is based on hourly data. Let's have a look at a visualization of our prediction and the true data.

```{r, fig.keep='all'}
par(oma=c(0, 0, 0, 5))
plot(hour.test_in$dteday,predicted.poisson.test.1,type="l",col="firebrick3",  xlab = "  ", ylab ="Number of Bikes used")
lines(hour.test_in$dteday,hour.test_truth,type="p",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
Here we see our predictions as a red line and the actual numbers in blue dots. 
As hourly numbers vary strongly, we want to see if this model would fit better on a daily basis.

```{r}
glm.poisson.2 <- glm(cnt ~ dteday + season + yr + mnth + holiday + weekday + workingday + weathersit + temp + hum + windspeed, data=day, family=quasipoisson)

glm.poisson.train.2 <- glm(formula = formula(glm.poisson.2), data = day.train)
```

making prediction on the test data 
```{r}
predicted.poisson.test.2 <- predict(glm.poisson.train.2, newdata = day.test_in)
```

```{r}
summary(predicted.poisson.test.2)
```

Checking RMSE
```{r}
rmse(day.test_truth, predicted.poisson.test.2)
```
Here we have a RMSE of 811, which is higher than above in total. But as we are looking at daily data here, with numbers up to above 8000, this seems to be a better fit.



```{r, fig.keep='all', echo=FALSE}
par(oma=c(0, 0, 0, 5))
plot(day.test_in$dteday,predicted.poisson.test.2,type="l",col="firebrick3", xlab = "  ", ylab ="Number of Bikes used") 
lines(day.test_in$dteday,day.test_truth,type="l",col="deepskyblue2")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,
       c("Prediction", "True Values"),col=c("firebrick3", "deepskyblue2"), pch=c(1, 2), lty=c(1,2))
```
When looking at the predictions (red) in comparison to the true data (blue), this model does look usable if one wants to plan when to take bikes out of circulation for maintenance or repairs.



To see, if a variable is not needed, the model was tested by dropping only one of the predictors for each run through (with daily and hourly data). But the best result was achieved by using all the predictors (except for the type of users, for the reasons mentioned at the beginning of this chapter).


#Support Vector Machine
##Load the data
```{r}
dt <- fread("../Data/hour.csv")
```

##check the data and format of the data tabel
```{r}
head(dt)
str(dt)
nrow(dt)
ncol(dt)

```

The columns detday and instant are of no use. detday is represented in the variables yr, mnth, hr. Instant is only the record index. Therefore they are removed. the target variable "cnt" is moved to the first position. 
As already mentioned in section before, the data at the 27 and 28 August is not complete and wrong. Therfore, we will drop it for the SVM as well.
```{r}
#drop the data for the 27 and 28 August 2011

dt <- subset(dt, dt$dteday != '2011-08-27')
dt <- subset(dt, dt$dteday != '2011-08-28')



#remove instant, dteday, casual and registered 
dt <- dt %>% select(-c(1,2, 15, 16))



#move cnt to the first position
dt <- dt %>% select(cnt, everything())


```

#Factor ordinal variables 
```{r}
# Season
dt$season <- factor(
  dt$season, levels = c(1,2,3,4),
  labels = c('Spring', 'Summer', 'Fall','Winter'),
  ordered = TRUE)
is.factor(dt$season)
# Year
dt$yr <- factor(dt$yr,
                levels = c(0, 1),
                labels = c(2011, 2012),
                ordered = TRUE)
is.factor(dt$yr)

# Month
dt$mnth <- factor(
  dt$mnth, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),
  labels = c('January','February', 'March', 'April',
             'May', 'June', 'July', 'August',
             'September','October', 'November', 'December'),
  ordered = TRUE)
is.factor(dt$mnth)

# Hour
dt$hr <- as.factor(dt$hr)
is.factor(dt$hr)

# Holiday
dt$holiday <- factor(dt$holiday,
                     levels = c(0, 1),
                     labels = c('Workday', 'Weekend'))


# Weekday
dt$weekday <- factor(
 dt$weekday,levels = c(1,2,3,4,5,6,0),
  labels = c('Monday','Tuesday','Wednesday','Thursday',
             'Friday', 'Saturday','Sunday'),
  ordered = TRUE)

# Workingday
dt$workingday <- factor(dt$workingday,
                        levels = c(0,1),
                        labels = c('Workday', 'Weekend'))

# Weathersit
dt$weathersit <- factor(
  dt$weathersit,
  levels = c(1,2,3,4),
  labels = c('Clear, Few clouds, Partly cloudy, Partly cloudy', 'Mist+Clouds',
             'Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist', 'Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds'))

dt %>% glimpse()
```

#Plot data
## Rentals by Season and Temperature
```{r}
colr = c("olivedrab3", 'yellow', 'orange', 'grey50')
# Rentals by Season & Temperature
options(repr.plot.width=12, repr.plot.height=8)
ggplot(dt, aes(temp, cnt, color = season)) + geom_point() +
  theme_bw(base_size = 20) + scale_color_manual(values = colr) +
  labs(title = "Rentals by Season & Temperature", x = "Temperature Celsius", y = "Total Rentals") +
  scale_y_continuous(labels = scales::label_comma())

```

## Rentals by Season, Temperature and Year
```{r}
#Rentals by Season & Temperature & Year
ggplot(dt, aes(temp, cnt, color = season)) + geom_point() +
  theme_bw(base_size = 20) + scale_color_manual(values = colr) +
  labs(title = "Rentals by Season, Temperature & Year", x = "Temperature Celsius Normalized", y = "Total Rentals") +
  scale_y_continuous(labels = scales::label_comma()) +
  facet_grid(~yr)
```

## Rentals by Humidity and Season, Rental by Windspeed and Season
```{r}

# Rentals by Humidity & Season 
a1 = ggplot(dt, aes(hum, cnt, color = season)) + geom_point() +
  theme_bw(base_size = 16) + scale_color_manual(values = colr) + facet_grid(~yr) +
  labs(title = "Rentals by Humidity & Season", x = "Humidity", y = "Total Rentals") +
  scale_y_continuous(labels = scales::label_comma()) + 
  theme(legend.position="bottom")
# Rentals by Windspeed & Season
b1 = ggplot(dt, aes(windspeed, cnt, color = season)) + geom_point() +
  theme_bw(base_size = 18) + scale_color_manual(values = colr) + facet_grid(~yr) +
  labs(title = "Rentals by Windspeed & Season", x = "Wind Speed", y = "Total Rentals") +
  scale_y_continuous(labels = scales::label_comma()) + 
  theme(legend.position="bottom")
options(repr.plot.width=16, repr.plot.height=8)
plot_grid(a1,b1, ncol = 2, nrow = 1)
```

## Rental and Temperature by Seasion
```{r}

ggplot(dt, aes(temp, cnt, color = season)) +
  geom_jitter(width = 0.25) + scale_color_manual(values = colr) +
  labs(y="Count of Rentals", title = "Rentals & Temperature by Season") +
  facet_grid(~season) + theme_bw(base_size = 18)
```

## Rentals and Temperature by Weather
```{r}
w = c('skyblue1','skyblue2','skyblue3','skyblue4','grey40')
ggplot(dt, aes(temp, cnt, color = weathersit)) + 
  geom_jitter(width = 0.25, show.legend = F) + 
  scale_color_manual(values = w) +
  labs(y="Count of Rentals", title = "Rentals & Temperature by Weather") + 
  facet_grid(~weathersit) + theme_bw(base_size = 18)
```

#Multicollinearity
Check for multicollinearity between the different response variables
```{r}
m <- dt
# make all data types numeric
cols <- c("season", "weathersit", "workingday", "holiday", "mnth", 
       "hr", "weekday","yr")
m[,cols] <- m %>% select(all_of(cols)) %>% lapply(as.numeric)

#checking multicolinearity with VIF
fit1 <- lm(cnt ~., data = m)
summary(fit1)
car::vif(fit1)
```

VIF is the Variance Inflation Factor and can be used to detect the presence of multicollinearity.
$$
VIF = \frac{1}{(1-R^2)}
$$
According to Zuur et al. 2010, a VIF > 10 shows multicollinearity. But also more restrictive values such as 3 or even 1 can be chosen. We start with 10. temp and atemp show a high multicolinearity. Therefore, the atemp, which represents the feeling temperature, will be removed from the original dataset.

```{r}
dt <- dt %>% select(everything(), -atemp)
```

#Train - test split
Split the data in a trainset with 75% of the data and a testset with 25% of the data
```{r}
index <- createDataPartition(dt$cnt, p = 0.75, list=F)
dt.train <- dt[index, ]
dt.test <- dt[-index, ]
```

# SVM model

## Kfold cross validation
```{r}
kfold <- trainControl(method = 'cv', number = 5)
```

## Linear kernel
Using a linear kernel an a k-fold cross validation
```{r}
set.seed(9876)

svm.lin.1 <- train(cnt ~ ., data = dt.train,
                   trControl = kfold, method='svmLinear2',
                   tuneGrid = data.frame(cost = c(1))
                   )
print(svm.lin.1)

```

Let's compare it to the linear kernel with the SVM implementation in the e1071 package.
```{r}
set.seed(9876)
svm.lin.2 <- svm(cnt~., data = dt.train, kernel = 'linear',
                 type = 'eps-regression',
                 degree = 1,
                 coef0 = 1,
                 cost = 1,
                 cross = 5)


rmse(dt.train$cnt, svm.lin.2$fitted)
summary(svm.lin.2)
```

The svm.lin.1 perform better and has a lower cost parameter. svm.lin.1 RMSE: 108.4244 and a cost of 1, compared to svm.lin.2 with a RMSE of 105.4065 and a cost of 1. 
The smaller the cost parameter, the more general the data is explained. If the cost parameter is chosen higher, the more specific it explains this data set. Therefore it is best to chose it as low as possible, with the best RSME value.If the cost value is higher, the model tend to overfit. The svm.lin.2 model has a better RMSE and is faster than the svm.lin.1. Therefore the svm.lin.2 is used for the prediction.


### Prediction with linear kernel
```{r}
set.seed(9876)
pred.linear <- predict(svm.lin.2, newdata=dt.test)
head(pred.linear)
rmse(dt.test$cnt, pred.linear)

```
RMSE of 102.3627 for the prediction

## Polynomial kernel
```{r}#
set.seed(9876)
svm.poly.1 <- train(cnt ~., data = dt.train,
                    trControl = kfold,
                    method = 'svmPoly'
                    )
summary(svm.poly.1)
print(svm.poly.1)

```
The svmPoly kernel uses the kernlab package. To tune the model, the degree, scale and C parameter can be changed to tune the model.

Lets try the polynomial svm with the e1071 package
```{r}
set.seed(9876)
#Polynomial: (gamma*u'*v + coef0)^degree
svm.poly.2 <- svm(cnt ~., data = dt.train, kernel = 'polynomial',
                  cross = 1, coef0 = 1, C = c(0.1, 0.25, 0.5, 1),
                  degree = 1)

svm.poly.2$coef0
svm.poly.2
rmse(dt.train$cnt, svm.poly.2$fitted)
svm.poly.2$degree



```
The svm.poly.2 achieved a RMSE of 107.25 with cost 1, degree 1 and coef 1. For the model testing, the svm.poly.2 will be used.

### Prediction with polynomaial kernel
```{r}
set.seed(9876)
pred.poly <- predict(svm.poly.2, newdata = dt.test)
head(pred.poly)


```
## Radial kernel
```{r}
set.seed(9876)

svm.rad.1 <- train(cnt ~., data=dt.train,
                   trConrol = kfold,
                   method = 'svmRadial')
summary(svm.rad.1)
print(svm.rad.1)

```
The svmRadial kernel uses also the kernlab package. The svmRadial kernel uses also the kernlab package. The radial kernel performed so far best, wit a RSME of  61.25279. The final values used for the model were sigma = 0.01062833 and C = 1.

### Prediction with radial kernel
```{r}
set.seed(9876)
pred.radial <- predict(svm.rad.1, newdata = dt.test)
head(pred.radial)
```

## Model comparision
Comparing the predictions based on their RMSE

```{r}
set.seed(9876)
#linear model
rmse(dt.test$cnt, pred.linear)

#polynomial model
rmse(dt.test$cnt, pred.poly)

#radial model
rmse(dt.test$cnt, pred.radial)
```
The radial kernel clearly outperformed the other two. the radial kernel has a RSME of 47.66508, the polynomial an RSME of 104.9329 and the linear kernel a RSME of 102.5022.
Therefor the winner of the support vecotor models is the one with the radial kernel. 


##1.1 Load data
```{r, echo= FALSE}
d <- setwd(dirname(getActiveDocumentContext()$path))
df <- read.csv("../Data/hour.csv")
```

Before starting to create NN models, all categorical variables are converted into a "one-hot" resp. binary variable. This is told to be best practice, as it should enhance the prediction performance let the model computing converge faster. Latter is also of great interest due to the long calculation time of NN models.
Also, the dependent variable is normalized.

##1.1 Decompose multi factor variable into single factor variable
```{r, echo= FALSE}
df$season_1 <- ifelse(df$season == "1", 1, 0)
df$season_2 <- ifelse(df$season == "2", 1, 0)
df$season_3 <- ifelse(df$season == "3", 1, 0)
df$season_4 <- ifelse(df$season == "4", 1, 0)
df$weathersit_1 <- ifelse(df$weathersit == "1", 1, 0)
df$weathersit_2 <- ifelse(df$weathersit == "2", 1, 0)
df$weathersit_3 <- ifelse(df$weathersit == "3", 1, 0)
df$weathersit_4 <- ifelse(df$weathersit == "4", 1, 0)
df$hr_0 <- ifelse(df$hr == "0", 1, 0)
df$hr_1 <- ifelse(df$hr == "1", 1, 0)
df$hr_2 <- ifelse(df$hr == "2", 1, 0)
df$hr_3 <- ifelse(df$hr == "3", 1, 0)
df$hr_4 <- ifelse(df$hr == "4", 1, 0)
df$hr_5 <- ifelse(df$hr == "5", 1, 0)
df$hr_6 <- ifelse(df$hr == "6", 1, 0)
df$hr_7 <- ifelse(df$hr == "7", 1, 0)
df$hr_8 <- ifelse(df$hr == "8", 1, 0)
df$hr_9 <- ifelse(df$hr == "9", 1, 0)
df$hr_10 <- ifelse(df$hr == "10", 1, 0)
df$hr_11 <- ifelse(df$hr == "11", 1, 0)
df$hr_12 <- ifelse(df$hr == "12", 1, 0)
df$hr_13 <- ifelse(df$hr == "13", 1, 0)
df$hr_14 <- ifelse(df$hr == "14", 1, 0)
df$hr_15 <- ifelse(df$hr == "15", 1, 0)
df$hr_16 <- ifelse(df$hr == "16", 1, 0)
df$hr_17 <- ifelse(df$hr == "17", 1, 0)
df$hr_18 <- ifelse(df$hr == "18", 1, 0)
df$hr_19 <- ifelse(df$hr == "19", 1, 0)
df$hr_20 <- ifelse(df$hr == "20", 1, 0)
df$hr_21 <- ifelse(df$hr == "21", 1, 0)
df$hr_22 <- ifelse(df$hr == "22", 1, 0)
df$hr_23 <- ifelse(df$hr == "23", 1, 0)
df$weekday_1 <- ifelse(df$weekday == "0", 1, 0)
df$weekday_2 <- ifelse(df$weekday == "1", 1, 0)
df$weekday_3 <- ifelse(df$weekday == "2", 1, 0)
df$weekday_4 <- ifelse(df$weekday == "3", 1, 0)
df$weekday_5 <- ifelse(df$weekday == "4", 1, 0)
df$weekday_6 <- ifelse(df$weekday == "5", 1, 0)
df$weekday_7 <- ifelse(df$weekday == "6", 1, 0)
df$mnth_1 <- ifelse(df$mnth == "1", 1, 0)
df$mnth_2 <- ifelse(df$mnth == "2", 1, 0)
df$mnth_3 <- ifelse(df$mnth == "3", 1, 0)
df$mnth_4 <- ifelse(df$mnth == "4", 1, 0)
df$mnth_5 <- ifelse(df$mnth == "5", 1, 0)
df$mnth_6 <- ifelse(df$mnth == "6", 1, 0)
df$mnth_7 <- ifelse(df$mnth == "7", 1, 0)
df$mnth_8 <- ifelse(df$mnth == "8", 1, 0)
df$mnth_9 <- ifelse(df$mnth == "9", 1, 0)
df$mnth_10 <- ifelse(df$mnth == "10", 1, 0)
df$mnth_11 <- ifelse(df$mnth == "11", 1, 0)
df$mnth_12 <- ifelse(df$mnth == "12", 1, 0)
```

##1.1 Normalize dependent var
```{r}
data <- df
data$cnt <- (df$cnt - min(df$cnt)) / (max(df$cnt) - min(df$cnt))
```

##1.1 Identifying near-zero variance variable
```{r}
nearZeroVar(data, saveMetrics = TRUE)
```

#3.1 Model 1
As a first approach to the data set, a several models are computed at once with a preset range of layer parameters. A slightly reduced training data volume is taken (65%). Each model is five-times cross validated. Also, the threshold is set to 0.1, which is rather a higher error tolerance value. Those first parameter settings are chosen in order to compute quantity rather than quality.
The result shown below suggests that there is no need of a third nor a second layer, and that the first layer doesn’t improve with more than four neurons in the first layer. The Root Mean Square Error is about 183. A prediction-vs-real data plot provides a visualization of the model performance. This plot suggests that there might be potential for improvement.


##3.1 Split data into train and test partition
```{r}
set.seed(123)
indices_1 <- createDataPartition(data$cnt, p=.65, list = F)

train_1 <- data %>% slice(indices_1)
test_1 <- data %>% slice(-indices_1)
```


##3.1 Model 1 - Set up
```{r}
set.seed(44)
tuGrid_1 <- expand.grid(.layer1=c(1,2,4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))

trCtrl_1 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final'
)
```

##3.1 Model 1 - Train and Compare
```{r}#
models_1 <- train(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.1,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_1,
  trControl = trCtrl_1
  )
```

##3.1 Model 1 - Save and Load
```{r}#
saveRDS(models_1, "neural_nets_models_1.rds")
```

```{r}
models_1 <-readRDS("neural_nets_models_1.rds")
```

```{r}
plot(models_1)
```

##3.1 Model 1 - Compute Prediction
```{r}
pred_1 <- compute(models_1$finalModel, test_1 %>% select(-cnt))
pred_1 <- pred_1$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_1 <- test_1$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
head(pred_1)
head(control_1)
```

##3.1 Model 1 - Root Mean Square
```{r}
sqrt(mean((control_1 - pred_1)^2))
```

##3.1 Model 1 - Plot Model Performance
```{r}
plot(control_1, pred_1, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

#3.2 Model 2
After the first investigation in the NN layer's behavior, the four-neurons model layout is taken to be computed again. This time, a higher data amount (80%) and an error threshold of 0.01 is set. The result of this more meticulous model shows already great improvement in RSME and the plot is also more satisfying. It is unclear if a model with higher complexity would have performed even better with a 0.01 threshold, but this is the trade-off for a much faster computing time.

##3.2 Split Data into Train and Test Partition
```{r}
set.seed(42)
indices_2 <- createDataPartition(data$cnt, p=.8, list = F)

train_2 <- data %>% slice(indices_2)
test_2 <- data %>% slice(-indices_2)
```

##3.2 Model 2 - Train
```{r}#
set.seed(42)
model_2 = neuralnet(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train_2, hidden = 4,linear.output = TRUE, threshold = 0.01, stepmax = 500000, lifesign.step = 1000, lifesign = "full")
```

##3.2 Model 2 - Save and Load
```{r}#
saveRDS(model_2, "neural_nets_model_2.rds")
```

```{r}
model_2 <-readRDS("neural_nets_model_2.rds")
```

##3.2 Model 2 - Compute Prediction
```{r}
pred_2 <- compute(model_2, test_2 %>% select(-cnt))
pred_2 <- pred_2$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_2 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.2 Model 2 - Root Mean Square
```{r}
sqrt(mean((control_2 - pred_2)^2))
```

##3.2 Model 2 - Plot Model Performance
```{r}
plot(control_2, pred_2, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

#3.4 Model 4 - all variable - low model complexity
Now more predictors are added to the model; the weekdays, the months and the wind speed. Again with a four-neuron model, the model is computed with a threshold of 0.05.
The results shows improvement in RSME (77) and thus suggest that there is valuable information in the added predictors. Again, the plot appear to be better.

##3.4 Model 4 - set up
```{r}
set.seed(41)
model_4 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(4),linear.output = TRUE, threshold = 0.05, stepmax = 600000, lifesign.step = 1000, lifesign = "full")
```

##3.4 Model 4 - Save and Load
```{r}#
saveRDS(model_4, "neural_nets_model_4.rds")
```

```{r}
model_4 <-readRDS("neural_nets_model_4.rds")
```

predict.nn train_2?
##3.4 Model 4 - Compute Prediction
```{r}
pred_4 <- compute(model_4, test_2 %>% select(-cnt))
pred_4 <- pred_4$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_4 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.4 Model 4 - Root Mean Square
```{r}
sqrt(mean((control_4 - pred_4)^2))
```

##3.4 Model 4 - Plot Model Performance
```{r}
plot(control_4, pred_4, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

confidence.interval(x, alpha = 0.05)

confidence.interval(model)

gwplot(x, rep = NULL, max = NULL, min = NULL, file = NULL,
selected.covariate = 1, selected.response = 1, highlight = FALSE,
type = "p", col = "black", ...)

gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")

#3.5 Models 5
With the new predictors added, the model might benefit from a new layer architecture. Again, a range of model is computed. But this time, more data (80%) and new predictors are added. The result suggest a model architecture of 7,3,0 neurons.

##3.5 Models 5 - Set up
```{r}
set.seed(44)
tuGrid_2 <- expand.grid(.layer1=c(4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))

trCtrl_2 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final',
)
```

##3.5 Models 5 - Train and Compare
```{r}
models_5 <- train(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.1,
  stepmax = 250000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_2,
  trControl = trCtrl_2
  )
```

```{r}#
saveRDS(models_5, "neural_nets_models_5.rds")
```

```{r}
models_5 <-readRDS("neural_nets_models_5.rds")
```

```{r}
plot(models_5)
```

##3.5 Model 5 - Compute Prediction
```{r}
pred_5 <- compute(models_5$finalModel, test_2 %>% select(-cnt))
pred_5 <- pred_5$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_5 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.5 Model 5 - Root Mean Square
```{r}
sqrt(mean((control_5 - pred_5)^2))
```

##3.5 Model 5 - Plot Model Performance
```{r}
plot(control_5, pred_5, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```
#Model 6
The 7,3,0 model is computed again but with higher “resolution”. The RSME drop down to 70.

##3.6 Model 6 - set up
```{r}#
set.seed(41)
model_6 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
model_6 <-readRDS("neural_nets_model_6.rds")
```

##3.6 Model 6 - Compute Prediction
```{r}
pred_6 <- compute(model_6, test_2 %>% select(-cnt))
pred_6 <- pred_6$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_6 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.6 Model 6 - Root Mean Square
```{r}
sqrt(mean((control_6 - pred_6)^2))
```

##3.6 Model 6 - Plot Model Performance
```{r}
plot(control_6, pred_6, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```



#3.9 Model 9 - all variable plus year
After some reflections about the data, one potential mistake stood out. Initially, the variable “year” was left out, since the data set covers only two years. But, as there is an increasing trend in the bike rental over the two years, there might be information in this predictor. 
The predictor “year” is added to the model 7,3,0. The result shows a great improvement in RSME with a drop to 49. The plot shows an overall better prediction behavior.

##3.9x Model 9x - set up
```{r}
set.seed(41)
model_9 = neuralnet(cnt ~ yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
model_9 <-readRDS("neural_nets_model_9.rds")
```

##3.9 Model 9 - Compute Prediction
```{r}
pred_9 <- compute(model_9, test_2 %>% select(-cnt))
pred_9 <- pred_9$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_9 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.9 Model 9 - Root Mean Square
```{r}
sqrt(mean((control_9 - pred_9)^2))
```

##3.9 Model 9 - Plot Model Performance
```{r}
plot(control_9, pred_9, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##3.10 Models 10 - Set up
```{r}
set.seed(41)
tuGrid_3 <- expand.grid(.layer1=c(6, 8, 10, 12, 14), .layer2=c(6, 8, 10), .layer3=c(2, 3, 4))

trCtrl_3 <- trainControl(
  method = 'repeatedcv',
  number = 3,
  repeats = 1,
  returnResamp = 'final',
)
```

##3.10 Models 10 - Train and Compare
```{r}
models_10c <- train(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.05,
  stepmax = 450000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_3,
  trControl = trCtrl_3
  )
```

##3.10 Model 10 - Save and Load
```{r}
saveRDS(models_10b, "neural_nets_models_10b.rds")
```

```{r}
plot(models_10)
```


#3.13 Model 13

##3.13 Model 13 - Train model
```{r}
set.seed(41)
model_13 = neuralnet(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(14,8,2),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
models_13 <-readRDS("neural_nets_model_13.rds")
```


#3.11 Model 11
After progressively adding the predictors to the model, the conclusion is that nearly all variables in the data set are useful as predictor, the remaining last two variables; “working day” and “holiday” are finally added to the model. Those two variables might help the model to understand the the case of rarer events, such as holiday or public holiday.
This two predictors slightly improved the model to an RSME of 46.

##3.11 Model 11 - Compute Prediction
```{r}
pred_11 <- compute(model_9x, test_2 %>% select(-cnt))
pred_11 <- pred_11$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_11 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.11 Model 11 - Root Mean Square
```{r}
sqrt(mean((control_11 - pred_11)^2))
```

##3.11 Model 11 - Plot Model Performance
```{r}
plot(control_11, pred_11, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##ANN Reflection & Conclusion:
- Being able to safe models with saveRDS() & readRDS() is a great relief when working with NN.
- It seems that R studio doesn't use the full computational potential of the CPU. It turned out that already an Intel quad core 8th gen mobile is able to process two model simultaneously, without any “noticeable” loss in computing speed. This come especially handy as training more elaborated NN models becomes heavily time consuming.
- Model with higher amount of neurons generally converged in fewer iteration steps. But in the end, as each iteration takes longer to be computed, the model takes more time to converged.
- One should adapt faster the number of neurons when increasing predictors.


#Optimization Problem

#Conclusion
The best Models based on the RMSE values
```{r}
#RMSE Linear Model

#RMSE GLM Model

#RMSE SVM Model wit a radial kernel and an RSME of[1] 47.66508
rmse(dt.test$cnt, pred.radial)

#RMSE NN Model
```


Justification, best model to find best maintenance window