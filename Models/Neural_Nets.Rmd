---
title: "Neural Net"
output: html_notebook
author: "Régis Michael Andreoli"
---


1.1 Set up - Load libraries & Data preparation

2.1 Split data into train and test partition

3.1 First Model - Train & compare - with diversed predictors
3.2 First Model - Test best model
3.2 First Model - Finetune best model
3.3 First Model - Test final model

4.1 Second Model - Train & compare - only as time series
4.2 Second Model - Test best model
4.3 Second Model - Finetune best model
4.4 Second Model - Test final model

5.1 Create SARIMA model and compare with best second model


--------------------------------------------------------------------------------
#1.1 Set up - Load libraries & Data preparation

##1.1 Load libraries
```{r, echo= FALSE}
library(tidyverse)
library(neuralnet)
library(caret)
library(rstudioapi)
library(rmarkdown)
library(dplyr)
```

##1.1 Load data
```{r, echo= FALSE}
d <- setwd(dirname(getActiveDocumentContext()$path))
df <- read.csv("../Data/hour.csv")
```

Before starting to create NN models, all categorical variables are converted into a "one-hot" resp. binary variable. This is told to be best practice, as it should enhance the prediction performance let the model computing converge faster. Latter is also of great interest due to the long calculation time of NN models.
Also, the dependent variable is normalized.

##1.1 Decompose multi factor variable into single factor variable
```{r, echo= FALSE}
df$season_1 <- ifelse(df$season == "1", 1, 0)
df$season_2 <- ifelse(df$season == "2", 1, 0)
df$season_3 <- ifelse(df$season == "3", 1, 0)
df$season_4 <- ifelse(df$season == "4", 1, 0)
df$weathersit_1 <- ifelse(df$weathersit == "1", 1, 0)
df$weathersit_2 <- ifelse(df$weathersit == "2", 1, 0)
df$weathersit_3 <- ifelse(df$weathersit == "3", 1, 0)
df$weathersit_4 <- ifelse(df$weathersit == "4", 1, 0)
df$hr_0 <- ifelse(df$hr == "0", 1, 0)
df$hr_1 <- ifelse(df$hr == "1", 1, 0)
df$hr_2 <- ifelse(df$hr == "2", 1, 0)
df$hr_3 <- ifelse(df$hr == "3", 1, 0)
df$hr_4 <- ifelse(df$hr == "4", 1, 0)
df$hr_5 <- ifelse(df$hr == "5", 1, 0)
df$hr_6 <- ifelse(df$hr == "6", 1, 0)
df$hr_7 <- ifelse(df$hr == "7", 1, 0)
df$hr_8 <- ifelse(df$hr == "8", 1, 0)
df$hr_9 <- ifelse(df$hr == "9", 1, 0)
df$hr_10 <- ifelse(df$hr == "10", 1, 0)
df$hr_11 <- ifelse(df$hr == "11", 1, 0)
df$hr_12 <- ifelse(df$hr == "12", 1, 0)
df$hr_13 <- ifelse(df$hr == "13", 1, 0)
df$hr_14 <- ifelse(df$hr == "14", 1, 0)
df$hr_15 <- ifelse(df$hr == "15", 1, 0)
df$hr_16 <- ifelse(df$hr == "16", 1, 0)
df$hr_17 <- ifelse(df$hr == "17", 1, 0)
df$hr_18 <- ifelse(df$hr == "18", 1, 0)
df$hr_19 <- ifelse(df$hr == "19", 1, 0)
df$hr_20 <- ifelse(df$hr == "20", 1, 0)
df$hr_21 <- ifelse(df$hr == "21", 1, 0)
df$hr_22 <- ifelse(df$hr == "22", 1, 0)
df$hr_23 <- ifelse(df$hr == "23", 1, 0)
df$weekday_1 <- ifelse(df$weekday == "0", 1, 0)
df$weekday_2 <- ifelse(df$weekday == "1", 1, 0)
df$weekday_3 <- ifelse(df$weekday == "2", 1, 0)
df$weekday_4 <- ifelse(df$weekday == "3", 1, 0)
df$weekday_5 <- ifelse(df$weekday == "4", 1, 0)
df$weekday_6 <- ifelse(df$weekday == "5", 1, 0)
df$weekday_7 <- ifelse(df$weekday == "6", 1, 0)
df$mnth_1 <- ifelse(df$mnth == "1", 1, 0)
df$mnth_2 <- ifelse(df$mnth == "2", 1, 0)
df$mnth_3 <- ifelse(df$mnth == "3", 1, 0)
df$mnth_4 <- ifelse(df$mnth == "4", 1, 0)
df$mnth_5 <- ifelse(df$mnth == "5", 1, 0)
df$mnth_6 <- ifelse(df$mnth == "6", 1, 0)
df$mnth_7 <- ifelse(df$mnth == "7", 1, 0)
df$mnth_8 <- ifelse(df$mnth == "8", 1, 0)
df$mnth_9 <- ifelse(df$mnth == "9", 1, 0)
df$mnth_10 <- ifelse(df$mnth == "10", 1, 0)
df$mnth_11 <- ifelse(df$mnth == "11", 1, 0)
df$mnth_12 <- ifelse(df$mnth == "12", 1, 0)
```

##1.1 Normalize dependent var
```{r}
data <- df
data$cnt <- (df$cnt - min(df$cnt)) / (max(df$cnt) - min(df$cnt))
```

##1.1 Identifying near-zero variance variable
```{r}
nearZeroVar(data, saveMetrics = TRUE)
```

#3.1 Model 1
As a first approach to the data set, a several models are computed at once with a preset range of layer parameters. A slightly reduced training data volume is taken (65%). Each model is five-times cross validated. Also, the threshold is set to 0.1, which is rather a higher error tolerance value. Those first parameter settings are chosen in order to compute quantity rather than quality.
The result shown below suggests that there is no need of a third nor a second layer, and that the first layer doesn’t improve with more than four neurons in the first layer. The Root Mean Square Error is about 183. A prediction-vs-real data plot provides a visualization of the model performance. This plot suggests that there might be potential for improvement.


##3.1 Split data into train and test partition
```{r}
set.seed(123)
indices_1 <- createDataPartition(data$cnt, p=.65, list = F)

train_1 <- data %>% slice(indices_1)
test_1 <- data %>% slice(-indices_1)
```


##3.1 Model 1 - Set up
```{r}
set.seed(44)
tuGrid_1 <- expand.grid(.layer1=c(1,2,4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))

trCtrl_1 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final'
)
```

##3.1 Model 1 - Train and Compare
```{r}#
models_1 <- train(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.1,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_1,
  trControl = trCtrl_1
  )
```

##3.1 Model 1 - Save and Load
```{r}#
saveRDS(models_1, "neural_nets_models_1.rds")
```

```{r}
models_1 <-readRDS("neural_nets_models_1.rds")
```

```{r}
plot(models_1)
```

##3.1 Model 1 - Compute Prediction
```{r}
pred_1 <- compute(models_1$finalModel, test_1 %>% select(-cnt))
pred_1 <- pred_1$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_1 <- test_1$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
head(pred_1)
head(control_1)
```

##3.1 Model 1 - Root Mean Square
```{r}
sqrt(mean((control_1 - pred_1)^2))
```

##3.1 Model 1 - Plot Model Performance
```{r}
plot(control_1, pred_1, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

#3.2 Model 2
After the first investigation in the NN layer's behavior, the four-neurons model layout is taken to be computed again. This time, a higher data amount (80%) and an error threshold of 0.01 is set. The result of this more meticulous model shows already great improvement in RSME and the plot is also more satisfying. It is unclear if a model with higher complexity would have performed even better with a 0.01 threshold, but this is the trade-off for a much faster computing time.

##3.2 Split Data into Train and Test Partition
```{r}
set.seed(42)
indices_2 <- createDataPartition(data$cnt, p=.8, list = F)

train_2 <- data %>% slice(indices_2)
test_2 <- data %>% slice(-indices_2)
```

##3.2 Model 2 - Train
```{r}#
set.seed(42)
model_2 = neuralnet(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train_2, hidden = 4,linear.output = TRUE, threshold = 0.01, stepmax = 500000, lifesign.step = 1000, lifesign = "full")
```

##3.2 Model 2 - Save and Load
```{r}#
saveRDS(model_2, "neural_nets_model_2.rds")
```

```{r}
model_2 <-readRDS("neural_nets_model_2.rds")
```

##3.2 Model 2 - Compute Prediction
```{r}
pred_2 <- compute(model_2, test_2 %>% select(-cnt))
pred_2 <- pred_2$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_2 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.2 Model 2 - Root Mean Square
```{r}
sqrt(mean((control_2 - pred_2)^2))
```

##3.2 Model 2 - Plot Model Performance
```{r}
plot(control_2, pred_2, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

#3.4 Model 4 - all variable - low model complexity
Now more predictors are added to the model; the weekdays, the months and the wind speed. Again with a four-neuron model, the model is computed with a threshold of 0.05.
The results shows improvement in RSME (77) and thus suggest that there is valuable information in the added predictors. Again, the plot appear to be better.

##3.4 Model 4 - set up
```{r}
set.seed(41)
model_4 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(4),linear.output = TRUE, threshold = 0.05, stepmax = 600000, lifesign.step = 1000, lifesign = "full")
```

##3.4 Model 4 - Save and Load
```{r}#
saveRDS(model_4, "neural_nets_model_4.rds")
```

```{r}
model_4 <-readRDS("neural_nets_model_4.rds")
```

predict.nn train_2?
##3.4 Model 4 - Compute Prediction
```{r}
pred_4 <- compute(model_4, test_2 %>% select(-cnt))
pred_4 <- pred_4$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_4 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.4 Model 4 - Root Mean Square
```{r}
sqrt(mean((control_4 - pred_4)^2))
```

##3.4 Model 4 - Plot Model Performance
```{r}
plot(control_4, pred_4, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

confidence.interval(x, alpha = 0.05)

confidence.interval(model)

gwplot(x, rep = NULL, max = NULL, min = NULL, file = NULL,
selected.covariate = 1, selected.response = 1, highlight = FALSE,
type = "p", col = "black", ...)

gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")

#3.5 Models 5
With the new predictors added, the model might benefit from a new layer architecture. Again, a range of model is computed. But this time, more data (80%) and new predictors are added. The result suggest a model architecture of 7,3,0 neurons.

##3.5 Models 5 - Set up
```{r}
set.seed(44)
tuGrid_2 <- expand.grid(.layer1=c(4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))

trCtrl_2 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final',
)
```

##3.5 Models 5 - Train and Compare
```{r}
models_5 <- train(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.1,
  stepmax = 250000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_2,
  trControl = trCtrl_2
  )
```

```{r}#
saveRDS(models_5, "neural_nets_models_5.rds")
```

```{r}
models_5 <-readRDS("neural_nets_models_5.rds")
```

```{r}
plot(models_5)
```

##3.5 Model 5 - Compute Prediction
```{r}
pred_5 <- compute(models_5$finalModel, test_2 %>% select(-cnt))
pred_5 <- pred_5$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_5 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.5 Model 5 - Root Mean Square
```{r}
sqrt(mean((control_5 - pred_5)^2))
```

##3.5 Model 5 - Plot Model Performance
```{r}
plot(control_5, pred_5, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```
#Model 6
The 7,3,0 model is computed again but with higher “resolution”. The RSME drop down to 70.

##3.6 Model 6 - set up
```{r}#
set.seed(41)
model_6 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
model_6 <-readRDS("neural_nets_model_6.rds")
```

##3.6 Model 6 - Compute Prediction
```{r}
pred_6 <- compute(model_6, test_2 %>% select(-cnt))
pred_6 <- pred_6$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_6 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.6 Model 6 - Root Mean Square
```{r}
sqrt(mean((control_6 - pred_6)^2))
```

##3.6 Model 6 - Plot Model Performance
```{r}
plot(control_6, pred_6, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```



#3.9 Model 9 - all variable plus year
After some reflections about the data, one potential mistake stood out. Initially, the variable “year” was left out, since the data set covers only two years. But, as there is an increasing trend in the bike rental over the two years, there might be information in this predictor. 
The predictor “year” is added to the model 7,3,0. The result shows a great improvement in RSME with a drop to 49. The plot shows an overall better prediction behavior.

##3.9x Model 9x - set up
```{r}
set.seed(41)
model_9 = neuralnet(cnt ~ yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
model_9 <-readRDS("neural_nets_model_9.rds")
```

##3.9 Model 9 - Compute Prediction
```{r}
pred_9 <- compute(model_9, test_2 %>% select(-cnt))
pred_9 <- pred_9$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_9 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.9 Model 9 - Root Mean Square
```{r}
sqrt(mean((control_9 - pred_9)^2))
```

##3.9 Model 9 - Plot Model Performance
```{r}
plot(control_9, pred_9, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##3.10 Models 10 - Set up
```{r}
set.seed(41)
tuGrid_3 <- expand.grid(.layer1=c(6, 8, 10, 12, 14), .layer2=c(6, 8, 10), .layer3=c(2, 3, 4))

trCtrl_3 <- trainControl(
  method = 'repeatedcv',
  number = 3,
  repeats = 1,
  returnResamp = 'final',
)
```

##3.10 Models 10 - Train and Compare
```{r}
models_10c <- train(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.05,
  stepmax = 450000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_3,
  trControl = trCtrl_3
  )
```

##3.10 Model 10 - Save and Load
```{r}
saveRDS(models_10b, "neural_nets_models_10b.rds")
```

```{r}
plot(models_10)
```




#3.11 Model 11
After progressively adding the predictors to the model, the conclusion is that nearly all variables in the data set are useful as predictor, the remaining last two variables; “working day” and “holiday” are finally added to the model. Those two variables might help the model to understand the the case of rarer events, such as holiday or public holiday.
This two predictors slightly improved the model to an RSME of 46.

##3.11 Model 11 - Compute Prediction
```{r}
pred_11 <- compute(model_11, test_2 %>% select(-cnt))
pred_11 <- pred_11$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_11 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.11 Model 11 - Root Mean Square
```{r}
sqrt(mean((control_11 - pred_11)^2))
```

##3.11 Model 11 - Plot Model Performance
```{r}
plot(control_11, pred_11, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

#3.12 Model y2

##3.12 Model y2 - Train model
```{r}
set.seed(41)
model_y2 = neuralnet(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(34,12,4),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

##3.13 Model 13 - Save and Load
```{r}
saveRDS(model_13, "neural_nets_model_13.rds")
```

```{r}
model_13 <-readRDS("neural_nets_model_13.rds")
```

##3.13 Model 13 - Compute Prediction
```{r}
pred_13 <- compute(model_13, test_2 %>% select(-cnt))
pred_13 <- pred_13$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_13 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

##3.13 Model 13 - Root Mean Square
```{r}
sqrt(mean((control_13 - pred_13)^2))
#[1] 45.00108
```




#SARIMA
data_ts <- ts(df$cnt)
x <- stl(data_ts, s.window = "periodic")
plot(x)


arima_model <- auto.arima(data_ts,ic="aic")
print(arima_model)

pred <- predict(model, n.ahead = 30)
plot(pred$pred)
lines(real_d, col = "orange")
