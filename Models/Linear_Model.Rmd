---
title: "Linear Model"
author: "Felix Bigler"
date: "11/12/2021"
output:
  html_document:
    toc: yes
    toc_float: no
    toc_depth: 4
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---
````{r, echo= FALSE}
# load library
library(rstudioapi)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(caret)
library(GGally)
library(mgcv)
```

````{r, echo=FALSE}
# set directory read data
setwd(dirname(getActiveDocumentContext()$path))
df <- read.csv("../Data/hour.csv")
`````
# Linear Regression Model
## Data Preperation for the Linear Model
### Set Categorical Variables as Factors
```{r}
df$season <- factor(df$season, levels = c("1", "2", "3", "4"), ordered = FALSE)
df$yr <- as.factor(df$yr)
df$mnth <- as.factor(df$mnth)
df$hr <- as.factor(df$hr)
df$holiday <- as.factor(df$holiday)
df$weekday <- as.factor(df$weekday)
df$workingday <- as.factor(df$workingday)
df$weathersit <- as.factor(df$weathersit)
```
### Re-Transform the Continious Weather Variables

As described in the data exploration chapter, the continuous weather variables have been normalized in the existing dataset. In order to make interpretation of the coefficients more straight forward in the linear model the variables will be re-transformed to the actual values as follows. For more advanced models the normalized variables may be the better choice to fit a model.
```{r}
df$temperature <- (df$temp * (39 + 8)) - 8
df$atemperature <- (df$atemp * (39 + 8)) - 8
df$humidity <- (df$hum * 100)
df$wind <- (df$windspeed * 67)
```
### Split data into train and test dataset

In the following the dataset will be split into a train and a test dataset. The partition will be used to evaluate the accuracy of the model when used on the test data.
````{r}
set.seed(123)
indices <- createDataPartition(df$cnt, p=.8, list = F)
train <- df %>% slice(indices)
test <- df %>% slice(-indices)
```

## Multiple Linear Regression Model with Continious Weather Predictors
First of all we want to include only the continuous variables in the model.
```{r}
lm.fit.0.train <- lm(log(cnt) ~ temperature + atemperature + humidity + wind,data=train)
summary(lm.fit.0.train)
```

### Simplification of Model lm.fit.0.train

There is evidence that the predictor temperature has no significant influence since the p-value is large with 0.7. However the feels like temperature (atemperature) is a result of the combination of the temperature, the humidity and the wind at a given time. Moreover, the prediction of future bike rental will depend from the weather forecast. In the forecast the feels like temperature is not always available. Therefore we try to fit the model without atemperature to simplify it and make it more practical for future usage.

```{r}
lm.fit.1.train <- lm(log(cnt) ~ temperature + humidity + wind,data=train)
summary(lm.fit.1.train)
```
Interpretation of Regression Coefficients:

- As we can see from the output above the predictor temperature and wind have both a positive influence upon the number of bike rentals. A increase of one unit (temperature) leads to a increase of 5.9 % rentals and 0.6 % when the wind is increased by one unit. The humidity on the other hand has a negative influence. If the humidity is increased by one unit the number of rentals decreases by 2.3 %.


### Accessing the model accuracy
We evaluate the accuracy of both models by testing them with the test data partition amd compare the RMSE values.

#### Compare RMSE Values
```{r}
control <- test$cnt
lm.fit.0.test = predict(lm.fit.0.train,newdata=test)
lm.fit.1.test = predict(lm.fit.1.train,newdata=test)

# take inverse of natural log to obtain RMSE with actual unit (number of bike rentals)
RMSE_0 <- sqrt(mean((control - exp(lm.fit.0.test))^2))
RMSE_1 <- sqrt(mean((control - exp(lm.fit.1.test))^2))

# calculate percentage error
percent_error_0 <- (RMSE_0/mean(control))*100
percent_error_1 <- (RMSE_1/mean(control))*100

print(paste("RMSE of lm.fit.0:",round(RMSE_0,1)))
print(paste("Percentage error of lm.fit.1:",round(percent_error_0,1),"%"))
print(paste("RMSE of lm.fit.0:",round(RMSE_1,1)))
print(paste("Percentage error of lm.fit.1:",round(percent_error_1,1),"%"))
```
- The RMSE values for both models are roughly 170 units as we can see from the output above. This corresponds to a percentage error of around 90 % of the predicted values if it is compared with the mean of all bike rentals.

## Multiple Linear Regression Model with Continious and Categorical Variables

```{r}
# Update the previous model lm.fit.1.train, including all the categorical variables.
lm.fit.2.train <- update(lm.fit.1.train,. ~ . + weathersit + hr + season + mnth + workingday + weekday + holiday + yr ,data=train)
formula(lm.fit.2.train)
```

### Simplification of Model lm.fit.2.train

In the next step we try fo find again a more simple and more practical model without reducing the predicting accuracy of the model significantly. With the drop1 function we test influence of the categorical variables with a F test as follows.

```{r}
drop1(lm.fit.2.train, test = "F")
```
There is evidence that the variables workingday an holiday have no significant influence according the results of the F-test. We therefore neglect this variables in the model lm.fit.3.train. The graphical analysis in the last chapter supports this step, showing that there is no influence of these variables.

```{r}
lm.fit.3.train <- update(lm.fit.1.train,. ~ . + weathersit + hr + season + mnth + weekday + yr ,data=train)
formula(lm.fit.3.train)
```
### Accessing the model accuracy

#### Compare RMSE Values
```{r}

lm.fit.2.test = predict(lm.fit.2.train,newdata=test)
lm.fit.3.test = predict(lm.fit.3.train,newdata=test)

# take inverse of natural log to calculate RMSE with actual unit (number of bike rentals)
RMSE_2 <- sqrt(mean((control - exp(lm.fit.2.test))^2))
RMSE_3 <- sqrt(mean((control - exp(lm.fit.3.test))^2))

# Calculate percentage error (deviation to the mean of number of bike rentals)
percent_error_2 <- (RMSE_2/mean(control))*100
percent_error_3 <-(RMSE_3/mean(control))*100

print(paste("RMSE of lm.fit.2:",round(RMSE_2,1)))
print(paste("Percentage error of lm.fit.2:",round(percent_error_2,1),"%"))
print(paste("RMSE of lm.fit.3:",round(RMSE_3,1)))
print(paste("Percentage error of lm.fit.3:",round(percent_error_3,1),"%"))

```
- The RMSE values for both 3 models are between 97 t0 110 units as we can see from the output above. This corresponds to a percentage error of around 51 to 59 % of the predicted values if it is compared with the mean of all bike rentals.


## Summary
As summary we want to answer following 4 questions regarding the linear regression model and its interpretation.

### Is there a relationship betweeen the independent variables and the response variable?

As stated above there is evidence, that the weather as well as the timely and seasonal variables has influence of the number of bike rentals according to the P-values for the continuous variables as well as the outcome of the F-test for the categorical variables.

### How strong is the relationship and how accurate is the model?

The predictors used in the model lm.fit.3 explain about 80 % of the variance in cnt (number of bike rentals) according to the R-squared values. If we look at the RMSE value of the lm.fit.4 we achieve a value of 110.5 which corresponds to a percentage error of around 60 %.

### How large is the effect of each predctor on cnt?

To answer this question we will plot the confidence interval of all the predictors.

```{r}
ggcoef(lm.fit.3.train,vline_color = "red", vline_linetype =  "solid", errorbar_color = "blue", errorbar_height = .25)
```
We can see that for the predictor month all dummy variables are crossing the zero line, indicating that this variable is not statistically significant. Therefore we could have dropped this predictor as well even though the p-values where low in the model examination. The evaluate this assumption we update the model and compare the two R-squared values.

```{r}
lm.fit.4.train <- update(lm.fit.1.train,. ~ . + weathersit + hr+ season + weekday ,data=df)
lm.fit.4.test = lm(formula(lm.fit.4.train),data=test)

summary(lm.fit.3.test)$r.squared
summary(lm.fit.4.test)$r.squared
```
In fact, the R-squared value decreases only marginally when the predictor month is not taken into account, even though the F-statistic has classified the predictor as significant.
Finally, it can be concluded that the temperature as well as the current time have the greatest effect upon the number of bike rentals, since the confidence intervals are the furthest away from zero.

### Are there Potential Problems of Fitting this data with a Linear Regression Model 

With the plot function, we try to examine our model in more detail by means of residual analysis.To to that we build a model with the entire dataset.
````{r}
lm.fit.0.full = lm(formula(lm.fit.4.train),data=df)
par(mfrow=c(2,2))
plot(lm.fit.0.full)
`````

- The residual vs fitted plot shows no clear pattern indicating that that relationship is linear. Also the smoother stays more or less on zero. In a clear non-linear situation this would have been the case.

- The QQ-plot shows that the residuals seem not to follow a normal distribution. This circumstance could be a sign that the linear model is not quite suitable to form a predictive model. We assume that this is due to the distribution of bike rentals. These are not normally distributed and could be considered as amount data.

- Checking the scale-location plot, it seems that the residuals are spread fairly randomly along the horizontal line. Altough the variance seems to decrease on the right hand side of the plot we consider this as "normal".

- In the residual vs leverage, one observation seems to lie outside of the cooks distance. It is the observation 5637 in the dataset. A closer look reveals an error in the dataset. On 27.08.2011 and 28.08.2011 the time seems to be incorrect and therefore the observation 5637 can be considered as an outlier. It would probably make sense to delete these two days from the dataset. We do this in the following step and form a new model.

````{r}
df2 <- df[!(df$dteday=="2011-08-27" | df$dteday=="2011-08-28"),]
lm.fit.1.full = lm(formula(lm.fit.4.train),data=df2)
plot(lm.fit.1.full, which = 5)
summary(lm.fit.0.full)$r.squared
summary(lm.fit.1.full)$r.squared
`````
- As we can see, all observations are lying now within the cook distance and the R-value also increases marginally when taking out the faulty date entries in a updated dataset.

In the next chapter we will use more advanced linear models to find a better fit for the data.

# General Additive Model (GAM)

In this chapter we would like to use the GAM model to address for possible non-linear relationships and improve the basic linear mode lm.fit.3.

## Check if model with non-linear function ist better

```{r}
gam_0_train <- gam(log(cnt) ~ temperature + humidity + wind,data=train)
gam_1_train <- gam(log(cnt) ~ s(temperature) + humidity + wind,data=train)
gam_2_train <- gam(log(cnt) ~ temperature + s(humidity) + wind,data=train)
gam_3_train <- gam(log(cnt) ~ temperature + humidity + s(wind),data=train)

anova(gam_0_train,gam_1_train,gam_2_train,gam_3_train,test="F")
```
- There is strong evidence that for all models applying the smoothing splines for the 3 continuous weather predictors is needed to improve the model. Consequently we will use the model applying the smoothing splines on all 3 weather predictor.

### Compare RMSE Values
```{r}
gam_4_train <- gam(log(cnt) ~ s(temperature) + s(humidity) + s(wind),data=train)

gam_0_test <- predict(gam_0_train,newdata=test)
gam_1_test <- predict(gam_1_train,newdata=test)
gam_2_test <- predict(gam_2_train,newdata=test)
gam_3_test <- predict(gam_3_train,newdata=test)
gam_4_test <- predict(gam_4_train,newdata=test)

# take inverse of natural log to calculate RMSE with actual unit (number of bike rentals)
RMSE_gam_0 <- sqrt(mean((control - exp(gam_0_test))^2))
RMSE_gam_1 <- sqrt(mean((control - exp(gam_1_test))^2))
RMSE_gam_2 <- sqrt(mean((control - exp(gam_2_test))^2))
RMSE_gam_3 <- sqrt(mean((control - exp(gam_3_test))^2))
RMSE_gam_4 <- sqrt(mean((control - exp(gam_4_test))^2))

print(paste("RMSE of gam_01(simple linear model):",round(RMSE_gam_0,1)))
print(paste("RMSE of gam_02(s(temperature)):",round(RMSE_gam_1,1)))
print(paste("RMSE of gam_02(s(humidity)):",round(RMSE_gam_2,1)))
print(paste("RMSE of gam_02(s(wind)):",round(RMSE_gam_3,1)))
print(paste("RMSE of gam_02(s(on all variables)):",round(RMSE_gam_4,1)))
```
- from the output above we can conclude that for all variables we get an improvement of the RMSE value when applying smoothing splines on it. Therefore we conclude that the relationship between predictors and response variable is not linear.

- for the gam_01 model we get the same result as for the lm.fit.1 model since it is nothing else than a simple linear model without any non linear functions in the gam() function.

## Add categorical variables to GAM Model gam_04
```{r}
gam_5_train <- update(gam_4_train,. ~ . + weathersit + hr + season + mnth + weekday + yr  ,data=train)
summary(gam_5_train)
```
- From the output above we can again conclude that there is strong evidence that the 3 continuous weather predictors have a non-linear effect according to the edf and p-values.

```{r}
gam_5_test <- predict(gam_5_train,newdata=test)
# take inverse of natural log to calculate RMSE with actual unit (number of bike rentals)
RMSE_gam_5 <- sqrt(mean((control - exp(gam_5_test))^2))
print(paste("RMSE of gam_5:",round(RMSE_gam_5,1)))
```
- We get a RSME of 91.5. Comparing to the lm.fit.3 with a RSME 97.4 we have an improvement of roughly 7.5 %.

- Below for the plots with the smoothing splines for each variable with and without the residuals.

## Summary

- It can be stated that using a GAA model with smoothing splines improves the model compared to a basic linear model. We have tried to add smoothing splines to the categorical variables but somehow we were not able to run this function. This could have improved the model even more.
```{r}
par(mfrow=c(1,3))
plot(gam_5_test,se=TRUE,col="blue")
```

```{r}
par(mfrow=c(1,3))
plot(gam_5_test,residuals =TRUE,cex=2,col="blue")
```

