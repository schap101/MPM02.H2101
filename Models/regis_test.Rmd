---
title: "MPM.02 Groupwork"
author: "Felix, Regis, Sofie, Philipp"
date: "06/01/2022"
output:
  html_document:
    toc: yes
    toc_float: no
    toc_depth: 4
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r, echo= FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load library
library(knitr)    # For knitting document and include_graphics function
library(png)
library(rstudioapi)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(rstudioapi)
library(caret)
library(mgcv)
library(GGally)
library(magrittr)
library(mosaicCore)
library(readr)
library(Metrics)
library(data.table)
library(e1071)
library(kernlab)
library(cowplot)
library(tidyverse)
library(neuralnet)
library(rstudioapi)
library(rmarkdown)
```

```{r, echo=FALSE}
# set directory read data
setwd(dirname(getActiveDocumentContext()$path))
df <- read.csv("C:/Users/regis/OneDrive/Dokumente/GitHub/MPM02.H2101/Data/hour.csv")
```



#Artificial Neural Network

##Data Preparation
###Load data


Before starting to create NN models, all categorical variables are converted into a "one-hot" resp. binary variable. This is told to be best practice, as it should enhance the prediction performance let the model computing converge faster. Latter is also of great interest due to the long calculation time of NN models.
Also, the dependent variable is normalized.

###Break multi factor variables down into single factor variables
```{r, echo=FALSE}
df$season_1 <- ifelse(df$season == "1", 1, 0)
df$season_2 <- ifelse(df$season == "2", 1, 0)
df$season_3 <- ifelse(df$season == "3", 1, 0)
df$season_4 <- ifelse(df$season == "4", 1, 0)
df$weathersit_1 <- ifelse(df$weathersit == "1", 1, 0)
df$weathersit_2 <- ifelse(df$weathersit == "2", 1, 0)
df$weathersit_3 <- ifelse(df$weathersit == "3", 1, 0)
df$weathersit_4 <- ifelse(df$weathersit == "4", 1, 0)
df$hr_0 <- ifelse(df$hr == "0", 1, 0)
df$hr_1 <- ifelse(df$hr == "1", 1, 0)
df$hr_2 <- ifelse(df$hr == "2", 1, 0)
df$hr_3 <- ifelse(df$hr == "3", 1, 0)
df$hr_4 <- ifelse(df$hr == "4", 1, 0)
df$hr_5 <- ifelse(df$hr == "5", 1, 0)
df$hr_6 <- ifelse(df$hr == "6", 1, 0)
df$hr_7 <- ifelse(df$hr == "7", 1, 0)
df$hr_8 <- ifelse(df$hr == "8", 1, 0)
df$hr_9 <- ifelse(df$hr == "9", 1, 0)
df$hr_10 <- ifelse(df$hr == "10", 1, 0)
df$hr_11 <- ifelse(df$hr == "11", 1, 0)
df$hr_12 <- ifelse(df$hr == "12", 1, 0)
df$hr_13 <- ifelse(df$hr == "13", 1, 0)
df$hr_14 <- ifelse(df$hr == "14", 1, 0)
df$hr_15 <- ifelse(df$hr == "15", 1, 0)
df$hr_16 <- ifelse(df$hr == "16", 1, 0)
df$hr_17 <- ifelse(df$hr == "17", 1, 0)
df$hr_18 <- ifelse(df$hr == "18", 1, 0)
df$hr_19 <- ifelse(df$hr == "19", 1, 0)
df$hr_20 <- ifelse(df$hr == "20", 1, 0)
df$hr_21 <- ifelse(df$hr == "21", 1, 0)
df$hr_22 <- ifelse(df$hr == "22", 1, 0)
df$hr_23 <- ifelse(df$hr == "23", 1, 0)
df$weekday_1 <- ifelse(df$weekday == "0", 1, 0)
df$weekday_2 <- ifelse(df$weekday == "1", 1, 0)
df$weekday_3 <- ifelse(df$weekday == "2", 1, 0)
df$weekday_4 <- ifelse(df$weekday == "3", 1, 0)
df$weekday_5 <- ifelse(df$weekday == "4", 1, 0)
df$weekday_6 <- ifelse(df$weekday == "5", 1, 0)
df$weekday_7 <- ifelse(df$weekday == "6", 1, 0)
df$mnth_1 <- ifelse(df$mnth == "1", 1, 0)
df$mnth_2 <- ifelse(df$mnth == "2", 1, 0)
df$mnth_3 <- ifelse(df$mnth == "3", 1, 0)
df$mnth_4 <- ifelse(df$mnth == "4", 1, 0)
df$mnth_5 <- ifelse(df$mnth == "5", 1, 0)
df$mnth_6 <- ifelse(df$mnth == "6", 1, 0)
df$mnth_7 <- ifelse(df$mnth == "7", 1, 0)
df$mnth_8 <- ifelse(df$mnth == "8", 1, 0)
df$mnth_9 <- ifelse(df$mnth == "9", 1, 0)
df$mnth_10 <- ifelse(df$mnth == "10", 1, 0)
df$mnth_11 <- ifelse(df$mnth == "11", 1, 0)
df$mnth_12 <- ifelse(df$mnth == "12", 1, 0)
```

###Normalize dependent variable
```{r, echo=FALSE}
data <- df
data$cnt <- (df$cnt - min(df$cnt)) / (max(df$cnt) - min(df$cnt))
```

##ANN Model 1
As a first approach to the data set, a few models are computed at once with a preset range of layer. A slightly reduced training data volume is taken (65%). Each model is five-times cross validated. Also, the threshold is set to 0.1, which is high error tolerance value. Those first parameter settings are chosen in order to compute quantity rather than quality.
The result shown below suggests that there is no need of a third nor a second layer, and that the first layer doesn’t improve with more than four neurons in the first layer. The Root Mean Square Error is about 183. A prediction-vs-real data plot provides a visualization of the model performance. This plot suggests that there might be potential for improvement. 

As the output neuron is set with "linear.output = TRUE", which means that the output is a continous number and not a factor.


###Split Data into Train and Test Partition
```{r}
set.seed(123)
indices_1 <- createDataPartition(data$cnt, p=.65, list = F)
train_1 <- data %>% slice(indices_1)
test_1 <- data %>% slice(-indices_1)
```


###Set up Parameter Range
```{r, eval=FALSE}
set.seed(44)
tuGrid_1 <- expand.grid(.layer1=c(1,2,4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))
trCtrl_1 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final')
```

###Train Models
```{r, eval=FALSE}
models_1 <- train(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train,
                  method = 'neuralnet',
                  metric = 'RMSE',
                  linear.output = TRUE,
                  threshold = 0.1,
                  lifesign.step = 1000,
                  lifesign = "full",
                  preProcess = c('center', 'scale'),
                  tuneGrid = tuGrid_1,
                  trControl = trCtrl_1)
```

###Save resp. Load Model
```{r, eval=FALSE}
saveRDS(models_1, "neural_nets_models_1.rds")
```

```{r}
models_1 <-readRDS("neural_nets_models_1.rds")
```

###Plot Models
```{r}
plot(models_1)
```

###Compute Prediction with best Model
```{r}
pred_1 <- compute(models_1$finalModel, test_1 %>% select(-cnt))
pred_1 <- pred_1$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_1 <- test_1$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r}
sqrt(mean((control_1 - pred_1)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_1, pred_1, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##ANN Model 2
After the first investigation in the NN layer's behavior, the four-neurons model layout is taken to be computed again. This time, a higher data amount (80%) and an error threshold of 0.01 is set. The result of this more meticulous model shows already great improvement in RSME and the plot is also more satisfying. It is unclear if a model with higher complexity would have performed even better with a 0.01 threshold, but this is the trade-off for a much faster computing time.

###Split Data into Train and Test Partition
```{r}
set.seed(42)
indices_2 <- createDataPartition(data$cnt, p=.8, list = F)

train_2 <- data %>% slice(indices_2)
test_2 <- data %>% slice(-indices_2)
```

###Train Model
```{r, eval=FALSE}
set.seed(42)
model_2 = neuralnet(cnt ~ hum + temp + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23, data = train_2, hidden = 4,linear.output = TRUE, threshold = 0.01, stepmax = 500000, lifesign.step = 1000, lifesign = "full")
```

###Save resp. Load Model
```{r, eval=FALSE}
saveRDS(model_2, "neural_nets_model_2.rds")
```

```{r}
model_2 <-readRDS("neural_nets_model_2.rds")
```

###Compute Prediction
```{r}
pred_2 <- compute(model_2, test_2 %>% select(-cnt))
pred_2 <- pred_2$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_2 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r}
sqrt(mean((control_2 - pred_2)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_2, pred_2, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##ANN Model 4: more Predictors, Layer 4/0/0, Threshold 0.05
Now more predictors are added to the model; the weekdays, the months and the wind speed. Again with a four-neuron model, the model is computed with a threshold of 0.05.
The results shows improvement in RSME (77) and thus suggest that there is valuable information in the added predictors. Again, the plot appear to be better.

###Train Model
```{r, eval=FALSE}
set.seed(41)
model_4 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(4),linear.output = TRUE, threshold = 0.05, stepmax = 600000, lifesign.step = 1000, lifesign = "full")
```

###Load model (computed in advance)
```{r}
model_4 <-readRDS("neural_nets_model_4.rds")
```

###Compute Prediction
```{r, echo=FALSE}
pred_4 <- compute(model_4, test_2 %>% select(-cnt))
pred_4 <- pred_4$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_4 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r, echo=FALSE}
sqrt(mean((control_4 - pred_4)^2))
```

###Plot Model Performance
```{r, echo=FALSE}
plot(control_4, pred_4, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

confidence.interval(x, alpha = 0.05)

confidence.interval(model)

gwplot(x, rep = NULL, max = NULL, min = NULL, file = NULL,
selected.covariate = 1, selected.response = 1, highlight = FALSE,
type = "p", col = "black", ...)

gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")

##ANN Models 5
With the new predictors added, the model might benefit from a new layer architecture. Again, a range of model is computed. But this time, more data (80%) and new predictors are added. The result suggest a model architecture of 7,3,0 neurons.

###Set up Parameter Range
```{r, eval=FALSE}
set.seed(44)
tuGrid_2 <- expand.grid(.layer1=c(4:8), .layer2=c(0,2,3,4), .layer3=c(0,2))

trCtrl_2 <- trainControl(
  method = 'repeatedcv',
  number = 5,
  repeats = 1,
  returnResamp = 'final',
)
```

###Train Models
```{r, eval=FALSE}
models_5 <- train(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.1,
  stepmax = 250000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_2,
  trControl = trCtrl_2)
```

```{r, echo=FALSE}
models_5 <-readRDS("neural_nets_models_5.rds")
```

###Plot Models
```{r}
plot(models_5)
```

###Compute Prediction
```{r}
pred_5 <- compute(models_5$finalModel, test_2 %>% select(-cnt))
pred_5 <- pred_5$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_5 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r}
sqrt(mean((control_5 - pred_5)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_5, pred_5, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```
##ANN Model 6: Layer 7/3/0, Threshold 0.01
The 7,3,0 model is computed again but with higher “resolution”. The RSME drops down to 70.

```{r, echo=FALSE, eval=FALSE}
set.seed(41)
model_6 = neuralnet(cnt ~ hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r, echo=FALSE}
model_6 <-readRDS("neural_nets_model_6.rds")
```

###Compute Prediction
```{r}
pred_6 <- compute(model_6, test_2 %>% select(-cnt))
pred_6 <- pred_6$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_6 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r}
sqrt(mean((control_6 - pred_6)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_6, pred_6, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```



## ANN Model 9: New Pred.: Year, Layer 7/3/0, Threshold 0.01
After some reflections about the data, one potential mistake stood out. Initially, the variable “year” was left out, since the data set covers only two years. But, as there is an increasing trend in the bike rental over the two years, there might be information in this predictor. 
The predictor “year” is added to the model 7,3,0. The result shows a great improvement in RSME with a drop to 49. The plot shows an overall better prediction behavior.

```{r, eval=FALSE, echo=FALSE}
set.seed(41)
model_9 = neuralnet(cnt ~ yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(7,3),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```

```{r}
model_9 <-readRDS("neural_nets_model_9.rds")
```

###Compute Prediction
```{r, echo=FALSE}
pred_9 <- compute(model_9, test_2 %>% select(-cnt))
pred_9 <- pred_9$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_9 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r, echo=FALSE}
sqrt(mean((control_9 - pred_9)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_9, pred_9, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##ANN Models 10

###Set up Parameter Range
```{r, eval=FALSE}
set.seed(41)
tuGrid_3 <- expand.grid(.layer1=c(6, 8, 10, 12, 14), .layer2=c(6, 8, 10), .layer3=c(2, 3, 4))

trCtrl_3 <- trainControl(
  method = 'repeatedcv',
  number = 3,
  repeats = 1,
  returnResamp = 'final',
)
```

###Train Models
```{r, eval=FALSE}
models_10c <- train(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2,
  method = 'neuralnet',
  metric = 'RMSE',
  linear.output = TRUE,
  threshold = 0.05,
  stepmax = 450000,
  lifesign.step = 1000,
  lifesign = "full",
  preProcess = c('center', 'scale'),
  tuneGrid = tuGrid_3,
  trControl = trCtrl_3
  )
```

```{r}
models_10 <-readRDS("neural_nets_models_10.rds")
```

```{r}
plot(models_10)
```


##ANN Model 13

###Train model
```{r, eval=FALSE}
set.seed(41)
model_13 = neuralnet(cnt ~ workingday + holiday + yr + hum + temp + windspeed + weathersit_1 + weathersit_2 + weathersit_3 + hr_1 + hr_2 + hr_3 + hr_4 + hr_5 + hr_6 + hr_7 + hr_8 + hr_9 + hr_10 + hr_11 + hr_12 + hr_13 + hr_14 + hr_15 + hr_16 + hr_17 + hr_18 + hr_19 + hr_20 + hr_21 + hr_22 + hr_23 + weekday_1 + weekday_2 + weekday_3 + weekday_4 + weekday_5 + weekday_6 + weekday_7 + mnth_1 + mnth_2 + mnth_3 + mnth_4 + mnth_5 + mnth_6 + mnth_7 + mnth_8 + mnth_9 + mnth_10 + mnth_11 + mnth_12, data = train_2, hidden = c(14,8,2),linear.output = TRUE, threshold = 0.01, stepmax = 750000, lifesign.step = 500, lifesign = "full")
```



##ANN Model 11
After progressively adding the predictors to the model, the conclusion is that nearly all variables in the data set are useful as predictor, the remaining last two variables; “working day” and “holiday” are finally added to the model. Those two variables might help the model to understand the the case of rarer events, such as holiday or public holiday.
This two predictors slightly improved the model to an RSME of 46.

```{r}
model_11 <-readRDS("neural_nets_model_11.rds")
```

###Compute Prediction
```{r, echo=FALSE}
pred_11 <- compute(model_11, test_2 %>% select(-cnt))
pred_11 <- pred_11$net.result * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
control_11 <- test_2$cnt * (max(df$cnt) - min(df$cnt)) + min(df$cnt)
```

###Root Mean Square
```{r}
sqrt(mean((control_11 - pred_11)^2))
```

###Plot Prediction against Real Data
```{r}
plot(control_11, pred_11, col='orange', cex = .3, pch=20, ylab = "predicted rating NN", xlab = "real rating")
abline(0,1)
```

##ANN Reflection & Conclusion:
* Being able to save models with saveRDS() & readRDS() is a great relief when working with NN.
* It seems that R studio doesn't use the full computational potential of the CPU. It turned out that already an Intel quad core 8th gen mobile is able to process two model simultaneously, without any “noticeable” loss in computing speed. This come especially handy as training more elaborated NN models becomes heavily time consuming.
* Model with higher amount of neurons generally converged in fewer iteration steps. But in the end, as each iteration takes longer to be computed, the model takes more time to converged.
* One should adapt faster the number of neurons when increasing predictors.
